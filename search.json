[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Note: this is an abbreviated syllabus. The full syllabus is available through Classmate and/or Canvas.\n\nInstructor Information\n\nDr. Samantha Seals\nOffice: 4/344\n\n\n\nMeeting Times and Location\n\nMW 1:00-2:15 pm\nClassroom: 51/152\n\n\n\nOffice Hours\n\nMonday: 2:30–4:30 pm (Central)\nWednesday: 2:30–4:30 pm (Central)\nThursday: 10:00 am–12:00 pm (Central)\nOther times by appointment\n\n\n\nGrading and Evaluation\nThe course grade will be determined as follows:\n\nAssignments (25%): Each module will include an activity for students to complete using Quarto and R. The resulting .html file should be submitted to the designated dropbox on Canvas by 11:59 pm on the specified date (see Canvas).\nProject (40%): You will work with your research team to complete a research project. There will be several check ins completed during the project period. The resulting .pdf file should be submitted to the designated dropbox on Canvas by 11:59 pm on the specified date (see Canvas).\nOUR Symposium (10%): Each team will put together a research poster based on the project. All research teams will present their poster at the symposium.\nFinal Exam (25%): The final exam will be a timed concepts-based exam. While there may be some calculations needed, you will not be processing raw data. The final exam will be in class 11:00 am-1:30 pm on April 30, 2025.\n\n\n\nLate Policy\nAssigments have due dates, however, the dropboxes will not close until the end of the semester. All students are automatically granted “extensions” without question.\nNote that if there is not a submission when I go to grade (after the initial deadline), I will assign a zero (0) and request that you submit the assignment when you are able to. This is only for record keeping purposes. There is no penalty for submitting late and a full grade will be given upon review of your submission.\nExtensions are not available for the projects or final exam.\n\n\nImportant University Dates\n\n\n\n\n\n\n\nDate\nEvent\n\n\n\n\nJan 8 (Mon)\nSpring begins.\n\n\nJan 14 (Tues)\nDrop/Add period ends.\n\n\nJan 20 (Mon)\nMartin Luther King’s birthday - campus closed.\n\n\nMar 17-23 (Mon-Fri)\nSpring Break - campus closed.\n\n\nApr 7 (Mon)\nWithdrawal deadline (automatic grade of “W”).\n\n\nMay 5 (Fri)\nLate withdrawal deadline (“W” or “WF”, see requirements below)."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-analysis-of-variance",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-analysis-of-variance",
    "title": "One-Way Analysis of Variance",
    "section": "Introduction: Analysis of Variance",
    "text": "Introduction: Analysis of Variance\n\nWe have previously discussed testing the difference between two groups.\n\nWhat about when there are three or more groups?\n\nWe will use a method called analysis of variance (ANOVA).\n\nThis method partitions the variance of the outcome into variance due to the groups and variance due to “other” factors.\n\nFun fact: the two-sample t-test is a special case of ANOVA.\n\nIf you perform ANOVA when comparing two means, you will obtain the same results as the two-sample t-test."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#hypotheses",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#hypotheses",
    "title": "One-Way Analysis of Variance",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nHypotheses all take the same form:\n\nH_0: \\ \\mu_1 = \\mu_2 = ... = \\mu_k\nH_1: at least one is different\n\nNote 1: you must fill in the “k” when writing hypotheses!\n\ne.g., if there are four means, your hypotheses are\n\nH_0: \\ \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\nH_1: at least one is different\n\n\nNote 2: ANOVA does not tell us which means are different, just if a general difference exists!"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-table",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-table",
    "title": "One-Way Analysis of Variance",
    "section": "ANOVA Table",
    "text": "ANOVA Table\n\nThe computations for ANOVA are more involved than what we’ve seen before.\nAn ANOVA table will be constructed in order to perform the hypothesis test.\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\ndf\nMean Squares\nF\n\n\n\n\nTreatment\nSSTrt\ndfTrt\nMSTrt\nF0\n\n\nError\nSSE\ndfE\nMSE\n\n\n\nTotal\nSSTot\ndfTot\n\n\n\n\n\n\nOnce this is put together, we can perform the hypothesis test.\n\nOur test statistic is the F0."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#the-f-distribution",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#the-f-distribution",
    "title": "One-Way Analysis of Variance",
    "section": "The F Distribution",
    "text": "The F Distribution\n\nThe F distribution is derived as the ratio of two variances.\n\nThe variances each have degrees of freedom: dfnumerator and dfdenominator\n\nThe F distribution’s shape depends on the df,"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations",
    "title": "One-Way Analysis of Variance",
    "section": "ANOVA Computations",
    "text": "ANOVA Computations\n\nAgain, here’s where we are headed with our computations:\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\ndf\nMean Squares\nF\n\n\n\n\nTreatment\nSSTrt\ndfTrt\nMSTrt\nF0\n\n\nError\nSSE\ndfE\nMSE\n\n\n\nTotal\nSSTot\ndfTot\n\n\n\n\n\n\nWe are partitioning the variance of our outcome into:\n\nVariance due to the grouping (treatment)\nVariance due to “other” factors (error)\n\nThink of this like a “catch all” for other sources of error – things we did not adjust for in our model."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-1",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-1",
    "title": "One-Way Analysis of Variance",
    "section": "ANOVA Computations",
    "text": "ANOVA Computations\n\nBefore we begin our computations, it would be helpful if we know\n\n \\bar{x}, \\ \\ n_i, \\ \\ \\bar{x}_i, \\ \\ s_i^2 \n\nwhere,\n\n\\bar{x} is the overall mean,\nn_i is the sample size for group i,\n\\bar{x}_i is the mean for group i, and\ns_i^2 is the variance for group i"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-2",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-2",
    "title": "One-Way Analysis of Variance",
    "section": "ANOVA Computations",
    "text": "ANOVA Computations\n\nWe begin our computations with the sums of squares:\n\n\n\\begin{align*}\n    \\text{SS}_{\\text{Trt}} &= \\sum_{i=1}^k n_i(\\bar{x}_i-\\bar{x})^2 \\\\\n    \\text{SS}_{\\text{E}} &= \\sum_{i=1}^k (n_i-1)s_i^2 \\\\\n    \\text{SS}_{\\text{Tot}} &= \\text{SS}_{\\text{Trt}} + \\text{SS}_{\\text{E}}\n\\end{align*}\n\n\nand each sum of squares has degrees of freedom:\n\n\\text{df}_{\\text{Trt}} = k-1 (number of groups – 1)\n\\text{df}_{\\text{E}} = n-k (overall sample size – number of groups)\n\\text{df}_{\\text{Tot}} = n-1 (overall sample size – 1) = \\text{df}_{\\text{Trt}} + \\text{df}_{\\text{E}}"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-3",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-3",
    "title": "One-Way Analysis of Variance",
    "section": "ANOVA Computations",
    "text": "ANOVA Computations\n\nOnce we have the sum of squares and corresponding degrees of freedom, we have the mean squares.\nGenerally, mean squares are the sum of square divided by the df,  \\text{MS}_X = \\frac{\\text{SS}_X}{\\text{df}_X}\nIn the case of one-way ANOVA, \n\\begin{align*}\n  \\text{MS}_{\\text{Trt}} &= \\frac{\\text{SS}_{\\text{Trt}}}{\\text{df}_{\\text{Trt}}} \\\\\n  \\text{MS}_{\\text{E}} &= \\frac{\\text{SS}_{\\text{E}}}{\\text{df}_{\\text{E}}}\n\\end{align*}\n\n\nNote that there is no \\text{MS}_{\\text{Tot}}!"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-4",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-4",
    "title": "One-Way Analysis of Variance",
    "section": "ANOVA Computations",
    "text": "ANOVA Computations\n\nFinally, we have the test statistic.\nGenerally, we construct an F for ANOVA by dividing the MS of interest by MS_{\\text{E}},  F_X = \\frac{\\text{MS}_X}{\\text{MS}_{\\text{E}}} \nIn one-way ANOVA, we are only constructing the F for treatment, F_0 = \\frac{\\text{MS}_{\\text{Trt}}}{\\text{MS}_{\\text{E}}}"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-5",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-computations-5",
    "title": "One-Way Analysis of Variance",
    "section": "ANOVA Computations",
    "text": "ANOVA Computations\n\nWe are finally done constructing our ANOVA table! As a reminder,\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\ndf\nMean Squares\nF\n\n\n\n\nTreatment\nSSTrt\ndfTrt\nMSTrt\nF0\n\n\nError\nSSE\ndfE\nMSE\n\n\n\nTotal\nSSTot\ndfTot"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-r-syntax",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#anova-r-syntax",
    "title": "One-Way Analysis of Variance",
    "section": "ANOVA: R Syntax",
    "text": "ANOVA: R Syntax\n\nWe can use the aov() and summary() functions.\n\n\nm &lt;- aov(continuous_variable ~ grouping_variable,\n         data = dataset_name)\nsummary(m)\n\n\nHowever, note that ANOVA is regression (and regression is ANOVA).\n\nWe can also use lm() to define the model and anova() to construct the ANOVA table.\n\n\n\nm &lt;- lm(continuous_variable ~ grouping_variable,\n         data = dataset_name)\nanova(m)"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental",
    "title": "One-Way Analysis of Variance",
    "section": "Example - Dental",
    "text": "Example - Dental\n\nProsthodontists specialize in the restoration of oral function, including the use of dental implants, veneers, dentures, and crowns. A researcher wanted to compare the shear bond strength of different repair kits for repairs of chipped porcelain veneer.\nHe randomly divided 20 porcelain specimens into four treatment groups: group 1 used the Cojet system, group 2 used the Silistor system, group 3 used the Cimara system, and group 4 used the Ceramic Repair system.\nAt the conclusion of the study, shear bond strength (in megapascals, MPa) was measured according to ISO 10477. The data are as follows,\n\n\nstrength &lt;- c(15.4, 12.9, 17.2, 16.6, 19.3,\n              17.2, 14.3, 17.6, 21.6, 17.5,\n               5.5,  7.7, 12.2, 11.4, 16.4,\n              11.0, 12.4, 13.5,  8.9,  8.1)\nsystem &lt;- c(rep(\"Cojet\",5), rep(\"Silistor\",5), rep(\"Cimara\",5), rep(\"Ceramic\",5))\ndata &lt;- tibble(system, strength)"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental-1",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental-1",
    "title": "One-Way Analysis of Variance",
    "section": "Example - Dental",
    "text": "Example - Dental\n\n\n\n  \n\n\n\n\nWhat is the continuous variable?\nWhat is the grouping variable?"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental-2",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental-2",
    "title": "One-Way Analysis of Variance",
    "section": "Example - Dental",
    "text": "Example - Dental\n\nOur first step will be to construct an ANOVA table for the data.\n\n\nm1 &lt;- aov(strength ~ system, data = data)\nsummary(m1)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nsystem       3  200.0   66.66   7.545 0.00229 **\nResiduals   16  141.4    8.84                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nm2 &lt;- lm(strength ~ system, data = data)\nanova(m2)"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#hypothesis-testing",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#hypothesis-testing",
    "title": "One-Way Analysis of Variance",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nHypotheses\n\nH_0: \\ \\mu_1 = \\mu_2 = ... = \\mu_k\nH_1: at least one mean is different\n\nTest Statistic\n\nF_0 (pulled from the ANOVA table)\n\np-Value\n\np = P[F_0 \\ge F_{\\text{df}_{\\text{Trt}}, \\text{df}_{\\text{E}}}]\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental-3",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental-3",
    "title": "One-Way Analysis of Variance",
    "section": "Example - Dental",
    "text": "Example - Dental\n\nUsing the dental data:\n\n\nsummary(m1)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nsystem       3  200.0   66.66   7.545 0.00229 **\nResiduals   16  141.4    8.84                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nDetermine if there is a difference in average strength between the groups. Test at the \\alpha=0.01 level."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental-4",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example---dental-4",
    "title": "One-Way Analysis of Variance",
    "section": "Example - Dental",
    "text": "Example - Dental\n\nHypotheses\n\nH_0: \\ \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4\nH_1: at least one mean is different\n\nTest Statistic and p-Value\n\nF_0 = 7.545\np = 0.002\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha; \\alpha = 0.01.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that there is a difference in average strength between the four groups."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-posthoc-testing",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-posthoc-testing",
    "title": "One-Way Analysis of Variance",
    "section": "Introduction: Posthoc Testing",
    "text": "Introduction: Posthoc Testing\n\nToday we have introduced ANOVA. Recall the hypotheses,\n\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_k\nH_1: at least one \\mu_i is different\n\nThe F test does not tell us which mean is different… only that a difference exists.\nIn theory, we could perform repeated t tests to determine pairwise differences.\n\nRecall that ANOVA is an extension of the t test… or that the t test is a special case of ANOVA.\nHowever, this will increase the Type I error rate (\\alpha)."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-posthoc-testing-1",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-posthoc-testing-1",
    "title": "One-Way Analysis of Variance",
    "section": "Introduction: Posthoc Testing",
    "text": "Introduction: Posthoc Testing\n\nRecall that the Type I error rate, \\alpha, is the probability of incorrectly rejecting H_0.\n\ni.e., we are saying there is a difference between the means when there is actually not a difference.\n\nSuppose we are comparing 5 groups.\n\nThis is 10 pairwise comparisons!!\n\n1-2, 1-3, 1-4, 1-5, 2-3, 2-4, 2-5, 3-4, 3-5, 4-5\n\nIf we perform repeated t tests under \\alpha=0.05, we are inflating the Type I error to 0.40! 😵"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-posthoc-testing-2",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-posthoc-testing-2",
    "title": "One-Way Analysis of Variance",
    "section": "Introduction: Posthoc Testing",
    "text": "Introduction: Posthoc Testing\n\nWhen performing posthoc comparisons, we can choose one of two paths:\n\nControl the Type I (familywise) error rate.\nDo not control the Type I error rate.\n\nNote that controlling the Type I error rate is more conservative than when we do not control it.\n\n“Conservative” = more difficult to reject.\n\nGenerally, statisticians:\n\ndo not control the Type I error rate if examining the results of pilot/preliminary studies that are exploring for general relationships.\ndo control the Type I error rate if examining the results of confirmatory studies and are attempting to confirm relationships observed in pilot/preliminary studies."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-posthoc-testing-3",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#introduction-posthoc-testing-3",
    "title": "One-Way Analysis of Variance",
    "section": "Introduction: Posthoc Testing",
    "text": "Introduction: Posthoc Testing\n\nThe posthoc tests we will learn:\n\nTukey’s test\n\nPerforms all pairwise tests and controls the Type I error rate\n\nFisher’s least significant difference\n\nPerforms all pairwise tests but does not control the Type I error rate\n\nDunnett’s test\n\nCompares each group to a control group and controls the Type I error rate\n\n\nCaution: we should only perform posthoc tests if we have determined that a general difference exists!\n\ni.e., we rejected when looking at the F test in ANOVA"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#example",
    "title": "One-Way Analysis of Variance",
    "section": "Example",
    "text": "Example\n\nRecall the dental example from earlier,\n\n\nm1 &lt;- aov(strength ~ system, data = data)\nsummary(m1)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nsystem       3  200.0   66.66   7.545 0.00229 **\nResiduals   16  141.4    8.84                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAre we justified in posthoc testing? (Recall: \\alpha=0.01)."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#tukeys-test",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#tukeys-test",
    "title": "One-Way Analysis of Variance",
    "section": "Tukey’s Test",
    "text": "Tukey’s Test\n\nTukey’s test allows us to do all pairwise comparisons while controlling \\alpha.\nThe underlying idea of the comparison:\n\nWe declare \\mu_i \\ne \\mu_j if |\\bar{y}_i - \\bar{y}_j| \\ge W, where  W = \\frac{q_{\\alpha}(k, \\text{df}_{\\text{E}})}{\\sqrt{2}} \\sqrt{\\text{MSE} \\left( \\frac{1}{n_i} + \\frac{1}{n_j} \\right)} \n\nq_{\\alpha}(k, \\text{df}_{\\text{E}}) is the critical value from the Studentized range distribution.\n\n\nWe will use the TukeyHSD() function.\n\nNote that this requires us to have created our model using the aov() function.\n\n\n\nm &lt;- aov(continuous_variable ~ grouping_variable, data = dataset_name)\nTukeyHSD(m)$grouping_variable"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#tukeys-test-1",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#tukeys-test-1",
    "title": "One-Way Analysis of Variance",
    "section": "Tukey’s Test",
    "text": "Tukey’s Test\n\nLet’s apply Tukey’s to the dental data.\n\n\nm &lt;- aov(strength ~ system, data = data)\nTukeyHSD(m, conf.level = 0.99)$system\n\n                  diff         lwr       upr       p adj\nCimara-Ceramic   -0.14 -7.04151507  6.761515 0.999845202\nCojet-Ceramic     5.50 -1.40151507 12.401515 0.044147158\nSilistor-Ceramic  6.86 -0.04151507 13.761515 0.010458208\nCojet-Cimara      5.64 -1.26151507 12.541515 0.038206781\nSilistor-Cimara   7.00  0.09848493 13.901515 0.008990873\nSilistor-Cojet    1.36 -5.54151507  8.261515 0.886304336\n\n\n\nWhich are significantly different at the \\alpha=0.01 level?"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#fishers-test",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#fishers-test",
    "title": "One-Way Analysis of Variance",
    "section": "Fisher’s Test",
    "text": "Fisher’s Test\n\nFisher’s allows us to test all pairwise comparisons but control the \\alpha.\nThe underlying idea of the comparison:\n\nWe declare \\mu_i \\ne \\mu_j if |\\bar{y}_i - \\bar{y}_j| \\ge \\text{LSD}, where  \\text{LSD} = t_{1-\\alpha/2, \\text{df}_\\text{E}} \\sqrt{\\text{MSE} \\left( \\frac{1}{n_i} + \\frac{1}{n_j} \\right)} \n\nWe will use the LSD.test() function from the agricolae package.\n\nNote that, like Tukey’s, this requires us to have created our model using the aov() function.\n\n\n\nlibrary(agricolae)\nresults &lt;- summary(m)\n(LSD.test(dataset_name$continuous_variable, # continuous outcome\n          dataset_name$grouping_variable, # grouping variable\n          results[[1]]$Df[2], # df_E\n          results[[1]]$`Mean Sq`[2], # MSE\n          alpha = alpha_level) # can omit if alpha = 0.05\n  )[5] # limit to only the pairwise comparison results"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#fishers-test-1",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#fishers-test-1",
    "title": "One-Way Analysis of Variance",
    "section": "Fisher’s Test",
    "text": "Fisher’s Test\n\nLet’s apply Fisher’s to the dental data.\n\n\nlibrary(agricolae)\nresults &lt;- summary(m)\nLSD.test(data$strength, \n         data$system, \n         results[[1]]$Df[2], \n         results[[1]]$`Mean Sq`[2],\n         alpha = 0.01)[5]\n\n$groups\n         data$strength groups\nSilistor         17.64      a\nCojet            16.28      a\nCeramic          10.78      b\nCimara           10.64      b\n\n\n\nWhich are significantly different at the \\alpha=0.01 level?"
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#dunnetts-test",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#dunnetts-test",
    "title": "One-Way Analysis of Variance",
    "section": "Dunnett’s Test",
    "text": "Dunnett’s Test\n\nDunnett’s test allows us to do all pairwise comparisons against only the control, while controlling \\alpha.\n\nThis has fewer comparisons than Tukey’s because we are not comparing non-control groups to one another.\ni.e., we are sharing the \\alpha between fewer comparisons now, which is preferred if we are not interested in the comparisons between non-control groups.\n\nThe underlying idea of the comparison:\n\nWe declare \\mu_i \\ne \\mu_j if |\\bar{y}_i - \\bar{y}_j| \\ge D, where  D = d_{\\alpha}(k-1, \\text{df}_{\\text{E}}) \\sqrt{\\text{MSE} \\left( \\frac{1}{n_i} + \\frac{1}{n_c} \\right)}, \n\nd_{\\alpha}(k-1, \\text{df}_{\\text{E}}) is the critical value from Dunnett’s table."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#dunnetts-test-1",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#dunnetts-test-1",
    "title": "One-Way Analysis of Variance",
    "section": "Dunnett’s Test",
    "text": "Dunnett’s Test\n\nWe will use the DunnettTest() function from the DescTools package to perform Dunnett’s test.\n\n\nlibrary(DescTools)\nDunnettTest(x=dataset_name$continuous_variable, \n            g=dataset_name$grouping_variable, \n            control = \"name of control group\")\n\n\nThe p-values are adjusted, so you can directly compare them to the specified \\alpha."
  },
  {
    "objectID": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#dunnetts-test-2",
    "href": "files/lectures/W06-L1-one-way-ANOVA-posthoc.html#dunnetts-test-2",
    "title": "One-Way Analysis of Variance",
    "section": "Dunnett’s Test",
    "text": "Dunnett’s Test\n\nLet’s apply Dunnett’s to the dental data.\n\nWe will treat “Ceramic” as the control group.\n\n\n\nlibrary(DescTools)\nDunnettTest(x=data$strength, \n            g=data$system, \n            control = \"Ceramic\")\n\n\n  Dunnett's test for comparing several treatments with a control :  \n    95% family-wise confidence level\n\n$Ceramic\n                  diff     lwr.ci    upr.ci   pval    \nCimara-Ceramic   -0.14 -5.0138317  4.733832 0.9997    \nCojet-Ceramic     5.50  0.6261683 10.373832 0.0259 *  \nSilistor-Ceramic  6.86  1.9861683 11.733832 0.0059 ** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWhich are significantly different at the \\alpha=0.01 level?"
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#introduction",
    "href": "files/lectures/W04-L1-dependent-t.html#introduction",
    "title": "Dependent t-Tests",
    "section": "Introduction",
    "text": "Introduction\n\nIn the last lecture, we reviewed statistical inference on two independent means.\n\nCI for \\mu_1-\\mu_2\nHypothesis test for \\mu_1-\\mu_2 (two-sample t-test)\n\nToday, we will focus on drawing conclusions about two dependent means.\n\nCI for \\mu_d = \\mu_1-\\mu_2\nHypothesis test for \\mu_d = \\mu_1-\\mu_2 (paired t-test)"
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#independent-vs.-dependent-data",
    "href": "files/lectures/W04-L1-dependent-t.html#independent-vs.-dependent-data",
    "title": "Dependent t-Tests",
    "section": "Independent vs. Dependent Data",
    "text": "Independent vs. Dependent Data\n\n\n\n\nIndependent data\n\n\nAn individual selected for one sample does not dictate which individual is to be in a second sample.\nIn the data, there is not a way to link the individuals in the sample.\n\n\n\n\n\n\n\n\nDependent data\n\n\nAn individual selected to be in one sample is used to determine the individual in the second sample.\nIn the data, there is a way to link the individuals in the sample.\n\n\n\n\n\nExamples:\n\nTwo sections of STA4173\nProject grades in one section of STA4173\nMale and female penguins\nPrices online vs. in store at Target"
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#estimating-the-difference-between-two-dependent-means",
    "href": "files/lectures/W04-L1-dependent-t.html#estimating-the-difference-between-two-dependent-means",
    "title": "Dependent t-Tests",
    "section": "Estimating the Difference Between Two Dependent Means",
    "text": "Estimating the Difference Between Two Dependent Means\n\nWe are now interested in comparing two dependent groups.\nWe assume that the two groups come from the same population and are going to examine the difference,\n\n\nd = y_{i, 1} - y_{i, 2}\n\n\nAfter drawing samples, we have the following,\n\n\\bar{d} estimates \\mu_d,\ns^2_d estimates \\sigma^2_d, and\nn is the sample size."
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#ci-for-the-difference-between-two-dependent-means",
    "href": "files/lectures/W04-L1-dependent-t.html#ci-for-the-difference-between-two-dependent-means",
    "title": "Dependent t-Tests",
    "section": "CI for the Difference Between Two Dependent Means",
    "text": "CI for the Difference Between Two Dependent Means\n\n\n\n\n\\mathbf{(1-\\boldsymbol\\alpha)100\\%} confidence interval for \\mathbf{\\boldsymbol\\mu_d}\n\n\n \\bar{d} \\pm t_{\\alpha/2} \\frac{s_d}{\\sqrt{n}} \n\nwhere t_{\\alpha/2} has n-1 degrees of freedom.\nTo construct this interval, we require either:\n\nthe differences to be normally distributed or\nthe sample size is sufficiently large (n \\ge 30)\n\n\n\n\n\n\n\nR syntax:\n\n\nt.test(dataset_name$variable1_name,\n       dataset_name$variable2_name, \n       paired = TRUE, \n       conf.level = confidence_level)"
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#ci-for-the-difference-between-two-dependent-means-1",
    "href": "files/lectures/W04-L1-dependent-t.html#ci-for-the-difference-between-two-dependent-means-1",
    "title": "Dependent t-Tests",
    "section": "CI for the Difference Between Two Dependent Means",
    "text": "CI for the Difference Between Two Dependent Means\n\nInsurance adjusters are concerned about the high estimates they are receiving for auto repairs from garage I compared to garage II.\n15 cars were taken to both garages for separate estimates of repair costs.\n\n\nlibrary(tidyverse)\ngarage &lt;- tibble(g1 = c(17.6, 20.2, 19.5, 11.3, 13.0, \n                        16.3, 15.3, 16.2, 12.2, 14.8,\n                        21.3, 22.1, 16.9, 17.6, 18.4), \n                 g2 = c(17.3, 19.1, 18.4, 11.5, 12.7, \n                        15.8, 14.9, 15.3, 12.0, 14.2, \n                        21.0, 21.0, 16.1, 16.7, 17.5))\n\n\nConstruct the 95% confidence interval for the average difference between the two garages.\nRemember the R syntax:\n\n\nt.test(dataset_name$variable1_name,\n       dataset_name$variable2_name, \n       paired = TRUE, \n       conf.level = confidence_level)"
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#ci-for-the-difference-between-two-dependent-means-2",
    "href": "files/lectures/W04-L1-dependent-t.html#ci-for-the-difference-between-two-dependent-means-2",
    "title": "Dependent t-Tests",
    "section": "CI for the Difference Between Two Dependent Means",
    "text": "CI for the Difference Between Two Dependent Means\n\nt.test(garage$g1, \n       garage$g2, \n       paired = TRUE, \n       conf.level = 0.95)\n\n\n    Paired t-test\n\ndata:  garage$g1 and garage$g2\nt = 6.0234, df = 14, p-value = 3.126e-05\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.3949412 0.8317254\nsample estimates:\nmean difference \n      0.6133333 \n\n\n\nThe 95% CI for \\mu_d, where d = x_{\\text{I}} - x_{\\text{II}} is (0.39, 0.83).\nFrom the problem statement:\n\nInsurance adjusters are concerned about the high estimates they are receiving for auto repairs from garage I compared to garage II.\n\nCan we say that estimates from garage I are higher than those from garage II?"
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#paired-t-test-for-two-dependent-means",
    "href": "files/lectures/W04-L1-dependent-t.html#paired-t-test-for-two-dependent-means",
    "title": "Dependent t-Tests",
    "section": "Paired t-Test for Two Dependent Means",
    "text": "Paired t-Test for Two Dependent Means\nHypotheses\n\nH_0: \\mu_d = \\mu_0 | H_0: \\mu_d \\le \\mu_0 | H_0: \\mu_d \\ge \\mu_0\nH_1: \\mu_d \\ne \\mu_0 | H_1: \\mu_d &gt; \\mu_0 | H_1: \\mu_d &lt; \\mu_0\n\nTest Statistic & p-Value\n\nt_0 = \\frac{\\bar{d}-\\mu_0}{\\frac{s_d}{\\sqrt{n}}}\np = 2 P[t \\ge |t_0|] | p = P[t \\ge |t_0|] | p = P[t \\le |t_0|]\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha.\n\nConclusion/Interpretation\n\n[Reject or fail to reject] H_0.\nThere [is or is not] sufficient evidence to suggest [alternative hypothesis in words]."
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#paired-t-test",
    "href": "files/lectures/W04-L1-dependent-t.html#paired-t-test",
    "title": "Dependent t-Tests",
    "section": "Paired t-Test",
    "text": "Paired t-Test\n\nR syntax:\n\n\nt.test(dataset_name$variable1_name, \n       dataset_name$variable2_name, \n       paired = TRUE, \n       mu = hypothesized_difference,\n       alternative = \"alternative\")\n\n\nImportant!!\n\nWe are estimating \\mu_1 - \\mu_2, but R is going to subtract in the order we state in the t.test() function."
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#paired-t-test-example",
    "href": "files/lectures/W04-L1-dependent-t.html#paired-t-test-example",
    "title": "Dependent t-Tests",
    "section": "Paired t-Test: Example",
    "text": "Paired t-Test: Example\n\nLet’s now formally determine if garage I’s estimates are higher than garage II’s. Test at the \\alpha=0.05 level.\nRecall the data,\n\n\ngarage &lt;- tibble(g1 = c(17.6, 20.2, 19.5, 11.3, 13.0, \n                        16.3, 15.3, 16.2, 12.2, 14.8,\n                        21.3, 22.1, 16.9, 17.6, 18.4), \n                 g2 = c(17.3, 19.1, 18.4, 11.5, 12.7, \n                        15.8, 14.9, 15.3, 12.0, 14.2, \n                        21.0, 21.0, 16.1, 16.7, 17.5))\n\n\nand the R syntax:\n\n\nt.test(dataset_name$variable1_name, \n       dataset_name$variable2_name, \n       paired = TRUE, \n       mu = hypothesized_difference,\n       alternative = \"alternative\")"
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#paired-t-test-example-1",
    "href": "files/lectures/W04-L1-dependent-t.html#paired-t-test-example-1",
    "title": "Dependent t-Tests",
    "section": "Paired t-Test: Example",
    "text": "Paired t-Test: Example\n\nt.test(garage$g1, \n       garage$g2,\n       paired = TRUE,\n       mu = 0,\n       alternative = \"greater\")\n\n\n    Paired t-test\n\ndata:  garage$g1 and garage$g2\nt = 6.0234, df = 14, p-value = 1.563e-05\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 0.4339886       Inf\nsample estimates:\nmean difference \n      0.6133333 \n\n\n\nAre the estimates from garage I significantly higher than those from garage II?"
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#paired-t-test-example-2",
    "href": "files/lectures/W04-L1-dependent-t.html#paired-t-test-example-2",
    "title": "Dependent t-Tests",
    "section": "Paired t-Test: Example",
    "text": "Paired t-Test: Example\nHypotheses\n\nH_0: \\ \\mu_{\\text{I}} \\le \\mu_{\\text{II}} OR \\mu_{d} \\le 0, where \\mu_d = \\mu_{\\text{I}} - \\mu_{\\text{II}}\nH_1: \\ \\mu_{\\text{I}} &gt; \\mu_{\\text{II}} OR \\mu_{d} &gt; 0\n\nTest Statistic and p-Value\n\nt_0 = 6.023\np &lt; 0.001\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nReject H_0.\nThere is sufficient evidence to suggest the estimates at garage I are higher than that of garage II."
  },
  {
    "objectID": "files/lectures/W04-L1-dependent-t.html#wrap-up",
    "href": "files/lectures/W04-L1-dependent-t.html#wrap-up",
    "title": "Dependent t-Tests",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we reviewed the dependent t-test.\n\nConfidence intervals\nHypothesis testing\n\nNext lectures:\n\nAssumptions on t-tests.\nWilcoxon rank sum\nWilcoxon signed rank"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#introduction",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#introduction",
    "title": "Multiple Regression",
    "section": "Introduction",
    "text": "Introduction\n\nWe have learned simple linear regression, y = \\beta_0 + \\beta_1 x + \\varepsilon\nToday, we will expand to multiple regression, which allows us to include multiple predictors, y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\varepsilon\n\nSimple linear regression is just a special case of multiple regression, where k=1.\n\nThe good news is that all things we learned for simple linear regression hold true for multiple regression! 😎"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#r-syntax",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#r-syntax",
    "title": "Multiple Regression",
    "section": "R Syntax",
    "text": "R Syntax\n\nWe again use the lm() function to define the model and summary() to see the results.\n\n\nm &lt;- lm(outcome ~ predictor_1 + predcitor_2 + ... + predictor_k, \n        data = dataset_name)\nsummary(m)\n\n\nAdditionally, we will find the confidence intervals for the \\beta_i using confint().\n\n\nconfint(m) # for 95% CI\nconfint(m, level = conf_level) # for other levels"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#interpretations",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#interpretations",
    "title": "Multiple Regression",
    "section": "Interpretations",
    "text": "Interpretations\n\nWe interpret coefficients much the same as in simple linear regression.\n\nIntercept: when [all predictors = 0], the average [outcome] is [\\hat{\\beta}_0].\nSlope: For a 1 [units of predictor i] increase in [predictor i], we expect [outcome] to [increase or decrease] by [|\\hat{\\beta}_i|] [units of outcome], after controlling for [all other predictors in the model].\n\nIf \\hat{\\beta}_i &gt; 0, there is an increase.\nIf \\hat{\\beta}_i &lt; 0, there is a decrease."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nA family doctor wishes to further examine the variables that affect their female patients’ total cholesterol.\nThey randomly select 14 female patients, measure their total cholesterol, and asks the patients to determine their average daily consumption of saturated fat.\nThe data is as follows,\n\n\nlibrary(tidyverse)\ndata &lt;- tibble(age = c(25, 25, 28, 32, 32, 32, 38, 42, 48, 51, 51, 58, 62, 65), \n               chol = c(180, 195, 186, 180, 210, 197, 239, 183, 204, 221, 243, 208, 228, 269), \n               fat = c(19, 28, 19, 16, 24, 20, 31, 20, 26, 24, 32, 21, 21, 30))\nhead(data, n = 2)"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-1",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-1",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nModel total cholesterol (y) as a function of age (x_1) and daily consumption of saturated fat (x_2).\n\n\nm &lt;- lm(chol ~ age + fat, data = data)\nsummary(m)\n\n\nCall:\nlm(formula = chol ~ age + fat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.874  -8.192   3.479   8.151  14.907 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  90.8415    15.9887   5.682 0.000142 ***\nage           1.0142     0.2427   4.179 0.001540 ** \nfat           3.2443     0.6632   4.892 0.000478 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.42 on 11 degrees of freedom\nMultiple R-squared:  0.8473,    Adjusted R-squared:  0.8196 \nF-statistic: 30.53 on 2 and 11 DF,  p-value: 3.239e-05\n\n\n\nThe resulting model is \\hat{\\text{cholesterol}}_i = 90.842 + 1.014 \\text{ age}_i + 3.244 \\text{ fat}_i"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-2",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-2",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nLet’s now interpret the slopes from the cholesterol example. \\hat{\\text{cholesterol}}_i = 90.842 + 1.014 \\text{ age}_i + 3.244 \\text{ fat}_i\n\nFor a 1 year increase in age, the cholesterol level is expected to increase by 1.014 mg/dL, after adjusting for the average daily consumption of saturated fat. \\vskip1em\nFor a 1 gram increase in average daily consumption of saturated fat, the total cholesterol is expected to increase by 3.244 mg/dL, after adjusting for age."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#significance-of-beta_i",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#significance-of-beta_i",
    "title": "Multiple Regression",
    "section": "Significance of \\beta_i",
    "text": "Significance of \\beta_i\n\nHypotheses\n\nH_0: \\ \\beta_i = 0\nH_1: \\ \\beta_i \\ne 0\n\nTest Statistic\n\nt_0 = \\frac{\\hat{\\beta}_i}{\\text{SE of }\\hat{\\beta}_i}\n\np-Value\n\np = 2 \\times P[t_{n-k-1} \\ge |t_0|]\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-3",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-3",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nWhich, if any, are significant predictors of cholesterol?\n\n\nsummary(m)\n\n\nCall:\nlm(formula = chol ~ age + fat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.874  -8.192   3.479   8.151  14.907 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  90.8415    15.9887   5.682 0.000142 ***\nage           1.0142     0.2427   4.179 0.001540 ** \nfat           3.2443     0.6632   4.892 0.000478 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.42 on 11 degrees of freedom\nMultiple R-squared:  0.8473,    Adjusted R-squared:  0.8196 \nF-statistic: 30.53 on 2 and 11 DF,  p-value: 3.239e-05"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-4",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-4",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{age}} = 0\nH_1: \\ \\beta_{\\text{age}} \\ne 0\n\nTest Statistic and p-Value\n\nt_0 = 4.179\np = 0.002\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha; \\alpha=0.05\n\nConclusion/Interpretation\n\nReject H_0.\nThere is sufficient evidence to suggest that there is a relationship between age and cholesterol."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-5",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-5",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{fat}} = 0\nH_1: \\ \\beta_{\\text{fat}} \\ne 0\n\nTest Statistic and p-Value\n\nt_0 = 4.892\np &lt; 0.001\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha; \\alpha=0.05\n\nConclusion/Interpretation\n\nReject H_0.\nThere is sufficient evidence to suggest that there is a relationship between fat and cholesterol."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-6",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-6",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nLet’s find the 95% confidence intervals for the regression coefficients.\n\n\nconfint(m)\n\n                 2.5 %     97.5 %\n(Intercept) 55.6507255 126.032306\nage          0.4800054   1.548405\nfat          1.7845942   4.703937\n\n\n\nThe 95% CI for \\beta_{\\text{age}} is (0.48, 1.55).\nThe 95% CI for \\beta_{\\text{fat}} is (1.78, 4.70)."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#reporting-results",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#reporting-results",
    "title": "Multiple Regression",
    "section": "Reporting Results",
    "text": "Reporting Results\n\nHow do I report regression models in the real world?\n\nI always give a table of \\hat{\\beta}_i, (95% CI), and p-values.\n\nIn our example,\n\n\n\n\nPredictor\nEstimate (95% CI)\np-Value\n\n\n\n\nAge\n1.01 (0.48, 1.55)\n0.002\n\n\nFat\n3.24 (1.78, 4.70)\n&lt; 0.001"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#significant-regression-line",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#significant-regression-line",
    "title": "Multiple Regression",
    "section": "Significant Regression Line",
    "text": "Significant Regression Line\n\nWe can use an F-test to test for a significant regression line.\n\nA significant regression line means that there is a non-zero slope among all slopes.\nThis makes use of an ANOVA table, however, we will not concern ourselves with the computation.\n\nThe results can be found at the bottom of the summary() output."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#significant-regression-line-1",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#significant-regression-line-1",
    "title": "Multiple Regression",
    "section": "Significant Regression Line",
    "text": "Significant Regression Line\n\nHypotheses\n\nH_0: \\ \\beta_1 = ... = \\beta_k = 0\nH_1: at least one is different\n\nTest Statistic\n\nF_0 (pulled from the summary() output)\n\np-Value\n\np = \\text{P}[F_{k, n-k-1} \\ge F_0]\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-7",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-7",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nIs this a significant regression line?\n\nIs at least one of the x_i a significant predictor?\nIs at least one slope non-zero?"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-8",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-8",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nHypotheses\n\nH_0: \\ \\beta_1 = \\beta_2 = 0\nH_1: at least one is different\n\nTest Statistic and p-Value\n\nF_0 = 30.53\np &lt; 0.001\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha=0.05.\n\nConclusion/Interpretation\n\nReject H_0.\nThere is sufficient evidence to suggest that at least one slope is non-zero."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#line-fit",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#line-fit",
    "title": "Multiple Regression",
    "section": "Line Fit",
    "text": "Line Fit\n\nWe can assess how well the regression model fits the data using R^2.  R^2 = \\frac{\\text{SS}_{\\text{Reg}}}{\\text{SS}_{\\text{Tot}}} \nThus, R^2 is the proportion of variation explained by the model (i.e., predictor set).\nR^2 \\in [0, 1]\n\nR^2 \\to 0 indicates that the model fits “poorly.”\nR^2 \\to 1 indicates that the model fits “well.”\nR^2 = 1 indicates that all points fall on the response surface."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#line-fit-1",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#line-fit-1",
    "title": "Multiple Regression",
    "section": "Line Fit",
    "text": "Line Fit\n\nRecall that the error term in ANOVA is the “catch all” …\n\nThe SSTot is constant for the outcome of interest.\nAs we add predictors to the model, we are necessarily increasing SSReg\n\nThe variance is moving from SSE to SSReg\n\n\nWe do not want to arbitrarily increase R^2, so we will use an adjusted version: R^2_{\\text{adj}} = 1 - \\frac{\\text{MS}_{\\text{E}}}{\\text{SS}_{\\text{Tot}}/\\text{df}_{\\text{Tot}}}"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#example-9",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#example-9",
    "title": "Multiple Regression",
    "section": "Example",
    "text": "Example\n\nThe R^2 and R^2_{\\text{adj}} both come out of the summary() function.\n\n\n\n\n\nR^2 is 0.847.\nR^2_{\\text{adj}} is 0.820 – 82.0% of the variability in cholesterol is explained by the model with age and fat."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#outliers",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#outliers",
    "title": "Multiple Regression",
    "section": "Outliers",
    "text": "Outliers\n\nDefinition: data values that are much larger or smaller than the rest of the values in the dataset.\nWe will look at the standardized residuals,  e_{i_{\\text{standardized}}} = \\frac{e_i}{\\sqrt{\\text{MSE}(1-h_i)}},  where\n\ne_i = y_i - \\hat{y}_i is the residual of the ith observation\nh_i is the leverage of the ith observation\n\nIf |e_{i_{\\text{standardized}}}| &gt; 2.5 \\ \\to \\  outlier.\nIf |e_{i_{\\text{standardized}}}| &gt; 3 \\ \\to \\  extreme outlier."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#outliers-1",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#outliers-1",
    "title": "Multiple Regression",
    "section": "Outliers",
    "text": "Outliers\n\nWe will use the rstandard() function to find the residuals, then we will count the outliers,\n\n\ndataset_name %&gt;% count(abs(rstandard(m))&gt;2.5)\n\n\nIn our example data,\n\n\ndata %&gt;% count(abs(rstandard(m))&gt;2.5)\n\n\n  \n\n\n\n\nThere are no outliers.\n\n(This is Happy Textbook Land!)"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#multicollinearity",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#multicollinearity",
    "title": "Multiple Regression",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nCollinearity/multicollinearity: a correlation between two or more predictor variables affects the estimation procedure.\nWe will use the variance inflation factor (VIF) to check for multicollinearity.  \\text{VIF}_j = \\frac{1}{1-R^2_j}, \nwhere\n\nj = the predictor of interest and j \\in \\{1, 2, ..., k \\},\nR^2_j results from regressing x_j on the remaining (k-1) predictors.\n\nWe say that multicollinearity is present if VIF &gt; 10."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#multicollinearity-1",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#multicollinearity-1",
    "title": "Multiple Regression",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nHow do we deal with multicollinearity?\n\nEasy answer: remove at least one predictor from the collinear set, then reassess VIF.\nMore complicated: how do we know which predictor should be the one removed?\n\n(We will likely need to consult with the research team.)"
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#multicollinearity-2",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#multicollinearity-2",
    "title": "Multiple Regression",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nWe will use the vif() function from the car package.\n\n\ncar::vif(m)\n\n\nThere will be a value for each predictor in the model.\nIn our cholesterol model,\n\n\ncar::vif(m)\n\n     age      fat \n1.117404 1.117404 \n\n\n\nNo multicollinearity is present."
  },
  {
    "objectID": "files/lectures/W09-L1-multiple-linear-regression.html#conclusions",
    "href": "files/lectures/W09-L1-multiple-linear-regression.html#conclusions",
    "title": "Multiple Regression",
    "section": "Conclusions",
    "text": "Conclusions\n\nThis is just scratching the surface for multiple regression.\nOther statistics courses go deeper into regression topics.\n\nCategorical predictors.\nInteraction terms.\nOther regression diagnostics.\nHow to handle non-continuous outcomes."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#introduction-correlation",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#introduction-correlation",
    "title": "Correlation and Simple Linear Regression",
    "section": "Introduction: Correlation",
    "text": "Introduction: Correlation\n\nBefore today, we discussed methods for comparing continuous outcomes across two or more groups.\nWe now will begin exploring the relationships between two continuous variables.\nWe will first focus on data visualization and correlation.\nThe we will quantify the relationship using regression analysis."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#scatterplot",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#scatterplot",
    "title": "Correlation and Simple Linear Regression",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nScatterplot or scatter diagram:\n\nA graph that shows the relationship between two quantitative variables measured on the same subject.\n\nEach individual in the dataset is represented by a point on the scatterplot.\nThe explanatory variable is on the x-axis and the response variable is on the y-axis.\nIt is super important for us to plot the data!\n\nPlotting the data is a first step in identifying issues or potential relationships."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#scatterplots",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#scatterplots",
    "title": "Correlation and Simple Linear Regression",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nPositive relationship: As x increases, y increases.\nNegative relationship: As x increases, y decreases."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nA golf pro wants to investigate the relation between the club-head speed of a golf club (measured in miles per hour) and the distance (in yards) that the ball will travel.\nThe pro uses a single model of club and ball, one golfer, and a clear, 70-degree day with no wind.\nThe pro records the club-head speed, measures the distance the ball travels, and collected the data below.\n\n\nlibrary(tidyverse)\ngolf &lt;- tibble(speed = c(100, 102, 103, 101, 105, 100, 99, 105),\n               distance = c(257, 264, 274, 266, 277, 263, 258, 275))\n  head(golf, n=3)"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nLike we have done before, we will graph using the ggplot2 package.\n\n\n\ngolf %&gt;% ggplot(aes(x = speed, y = distance)) + \n  geom_point(size = 5) + # plot points; make dots bigger\n  labs(x = \"Club Head Speed (mph)\",\n       y = \"Distance (yards)\") + # define labels \n  theme_bw() # change background color"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#correlation",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#correlation",
    "title": "Correlation and Simple Linear Regression",
    "section": "Correlation",
    "text": "Correlation\n\nCreating the scatterplot allows us to visualize a potential relationship.\n\ne.g., we know from the scatterplot for the golf data, as speed increases, distance increases.\n\nNow, let’s talk about how to quantify that relationship.\n\nInitial quantification: correlation.\nFurther quantification: regression."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#correlation-1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#correlation-1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Correlation",
    "text": "Correlation\n\nCorrelation: A measure of the strength and direction of the linear relationship between two quantitative variables.\n\n\\rho represents the population correlation coefficient.\nr represents the sample correlation coefficient.\n\nCorrelation is bounded to [-1, 1].\n\nr=-1 represents perfect negative correlation.\nr=1 represents perfect positive correlation.\nr=0 represents no correlation."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#pearsonss-correlation",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#pearsonss-correlation",
    "title": "Correlation and Simple Linear Regression",
    "section": "Pearsons’s Correlation",
    "text": "Pearsons’s Correlation\n\nPearson’s correlation coefficient:\n\nr = \\frac{\\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s_x} \\right)\\left( \\frac{y_i - \\bar{y}}{s_y} \\right)}{n-1}\n\nx_i is the ith observation of x; y_i is the ith observation of y\n\\bar{x} is the sample mean of x; \\bar{y} is the sample mean of y\ns_x is the sample standard deviation of x; s_y is the sample standard deviation of y\nn is the sample size (number of paired observations)"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#pearsonss-correlation-1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#pearsonss-correlation-1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Pearsons’s Correlation",
    "text": "Pearsons’s Correlation\n\nIn R, we will use the cor() function to find the correlation.\n\nWe can plug in the two specific variables we are interested in.\nWe can also plug in the dataset we are working with and all pairwise correlations will be reporeted.\n\n(This will be useful later!)\n\n\n\n\ncor(dataset_name$variable_1, dataset_name$variable_2) # request correlation between two values \ncor(dataset_name) # request correlation from a tibble (dataset)"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-2",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-2",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nLet’s find the correlation of the golf data.\nFirst, let’s plug in the specific variables we are interested in.\n\n\ncor(golf$distance, golf$speed)\n\n[1] 0.9386958\n\n\n\nLooking at the correlation coefficient for distance and speed, r = 0.94.\n\nThis is a strong positive correlation."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-3",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-3",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nNow, let’s plug the whole dataset in.\n\n\ncor(golf)\n\n             speed  distance\nspeed    1.0000000 0.9386958\ndistance 0.9386958 1.0000000\n\n\n\nNotes:\n\nThe correlation between a variable and itself is 1.\n\nWe can see this above: r_{\\text{speed, speed}} = 1 and r_{\\text{distance, distance}} = 1.\n\nThe correlation between x and y is the same as the correlation between y and x.\n\nWe can see this above: r_{\\text{speed, distance}} = r_{\\text{distance, speed}}"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#spearmans-correlation",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#spearmans-correlation",
    "title": "Correlation and Simple Linear Regression",
    "section": "Spearman’s Correlation",
    "text": "Spearman’s Correlation\n\nAn assumption for Pearson’s correlation is that both x and y are normally distributed.\nWhat do we do when we do not meet the normality assumption?\nSpearman’s Correlation: A measure of the strength and direction of the monotone relationship between two variables.\n\nSpearman’s correlation only assumes that the data is ordinal.\nIt does not work for nominal data!\n\nSpearman’s correlation is interpreted the same as Pearson’s correlation.\nTo find Spearman’s correlation, the following algorithm is followed:\n\nRank the x and y values.\n\n(x, y) \\to (R_x, R_y)\n\nFind Pearson’s correlation for the ranked data."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#spearmans-correlation-1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#spearmans-correlation-1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Spearman’s Correlation",
    "text": "Spearman’s Correlation\n\nWe will again use the cor() function, but now we will specify Spearman.\n\n\ncor(dataset_name$variable_1, dataset_name$variable_2, method = \"spearman\") \ncor(dataset_name, method = \"spearman\")"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-4",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-4",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nLet’s find the Spearman correlation of the golf data.\n\n\ncor(golf$distance, golf$speed, method = \"spearman\")\n\n[1] 0.9277782\n\n\n\nSpearman’s correlation is r_{\\text{S}} = 0.93.\n\nThis is still a strong, positive correlation.\n\nCompare this to Pearson’s correlation, r = 0.94.\n\nIn this dataset, Spearman’s correlation is not vastly different.\nWhile we see that Spearman’s correlation is smaller than Pearson’s correlation, this is not always the case."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#conclusions",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#conclusions",
    "title": "Correlation and Simple Linear Regression",
    "section": "Conclusions",
    "text": "Conclusions\n\nCorrelation allows us to quantify the relationship between two variables without units.\n\nIt does not matter if x and y have the same units! Correlation is unitless.\n\nThe closer to -1 or 1, the stronger the relationship.\n\nAs correlation approaches -1 or 1, x is better at predicting y.\n\nFor fun: play Guess the Correlation."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#introduction-simple-linear-regression",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#introduction-simple-linear-regression",
    "title": "Correlation and Simple Linear Regression",
    "section": "Introduction: Simple Linear Regression",
    "text": "Introduction: Simple Linear Regression\n\nWe have discussed quantifying the relationship between two continuous variables.\nRecall that the correlation describes the strength and the direction of the relationship.\n\nPearson’s correlation: describes the linear relationship; assumes normality of both variables.\nSpearman’s correlation: describes the monotone relationship; assumes both variables are at least ordinal.\n\nFurther, recall that correlation is unitless and bounded to [-1, 1]."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#introduction-simple-linear-regression-1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#introduction-simple-linear-regression-1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Introduction: Simple Linear Regression",
    "text": "Introduction: Simple Linear Regression\n\nNow we will discuss a different way of representing/quantifying the relationship.\n\nWe will now construct a line of best fit, called the ordinary least squares (OLS) regression line.\n\nUsing simple linear regression, we will model y (the outcome) as a function of x (the predictor).\n\nThis is called simple linear regression becaause there is only one predictor.\nNext week, we will venture into multiple regression, which has mulitple predictors."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression",
    "title": "Correlation and Simple Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nPopulation regression line:\n\n y = \\beta_0 + \\beta_1 x + \\varepsilon \n\n\\beta_0 is the y-intercept.\n\\beta_1 is the slope describing the relationship between x and y.\n\\varepsilon (estimated by e) is the error term; remember, from ANOVA (😱):\n\n\\varepsilon \\overset{\\text{iid}}{\\sim} N(0, \\sigma^2)"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression-1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression-1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nPopulation regression line:\n\n y = \\beta_0 + \\beta_1 x + \\varepsilon \n\nSample regression line:\n\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + e\n\n\\hat{y} estimates y.\n\\hat{\\beta}_0 estimates \\beta_0.\n\\hat{\\beta}_1 estimates \\beta_1.\ne estimates \\varepsilon."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression-2",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression-2",
    "title": "Correlation and Simple Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nWe first calculate the slope, \\hat{\\beta}_1\n\n \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n}}{\\sum_{i=1}^n x_i^2 - \\frac{\\left(\\sum_{i=1}^n x_i\\right)^2}{n}} = r \\frac{s_y}{s_x} \n\nr is the Pearson correlation,\ns_x is the standard deviation of x, and\ns_y is the standard deviation of y"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression-3",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression-3",
    "title": "Correlation and Simple Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nThen we can calculate the intercept, \\hat{\\beta}_0\n\n \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} where\n\n\\bar{x} is the mean of x, and\n\\bar{y} is the mean of y."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression-4",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#simple-linear-regression-4",
    "title": "Correlation and Simple Linear Regression",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\n\nWe already know the syntax for modeling!\n\nANOVA = regression 😎\n\nWe will use the lm() function to define the model and summary() to see the results.\n\n\nm &lt;- lm(outcome_variable ~ predictor_variable, data = dataset_name)\nsummary(m)"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-5",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-5",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nRecall the golf data,\n\n\nlibrary(tidyverse)\ngolf &lt;- tibble(speed = c(100, 102, 103, 101, 105, 100, 99, 105), \n               distance = c(257, 264, 274, 266, 277, 263, 258, 275))\nhead(golf, n=3)"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-6",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-6",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nLet’s now construct the simple linear regression model.\n\nWe want to model distance as a function of speed.\n\n\n\nm &lt;- lm(distance ~ speed, data = golf)\nsummary(m)\n\n\nCall:\nlm(formula = distance ~ speed, data = golf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8136 -2.0195  0.3542  2.0619  3.6881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -55.7966    48.3713  -1.154  0.29257    \nspeed         3.1661     0.4747   6.670  0.00055 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.883 on 6 degrees of freedom\nMultiple R-squared:  0.8811,    Adjusted R-squared:  0.8613 \nF-statistic: 44.48 on 1 and 6 DF,  p-value: 0.0005498\n\n\n\nThis results in the model,\n\n\\hat{\\text{distance}} = -55.80 + 3.17 \\text{ speed}"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#interpretations",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#interpretations",
    "title": "Correlation and Simple Linear Regression",
    "section": "Interpretations",
    "text": "Interpretations\n\nWe need to provide interpretations of\nInterpretation of the slope, \\hat{\\beta}_1\n\nFor a [k] [units of x] increase in [x], we expect [y] to [increase or decrease] by [k \\times |\\hat{\\beta}_1|] [units of y].\n\nInterpretation of the y-intercept, \\hat{\\beta}_0\n\nWhen [x] is 0, we expect the mean of [y] to be [\\hat{\\beta}_0].\nCaveat:\n\nIt does not always make sense for x=0. In this situation, we do not interpret the y-intercept.\nSome applications (e.g., psychology) will center the x variable around its mean. Then, when x=0, we are interpreting in terms of the “average value of x.”"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-7",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-7",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nRecall the regression line from the golf pro example, \\hat{\\text{distance}} = -55.80 + 3.17 \\text{ speed}\n\nFor a 1 mph increase in club-head speed, we expect the distance the golf ball travels to increase by 3.17 yards.\nFor a 5 mph increase in club-head speed, we expect the distance the golf ball travels to increase by 15.83 yards.\nWhen the club-head speed is 0 mph, the average distance the golf ball travels is -55.797 yards."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#hypothesis-testing-for-beta_1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#hypothesis-testing-for-beta_1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Hypothesis Testing for \\beta_1",
    "text": "Hypothesis Testing for \\beta_1\n\nFirst, the residual:\n\ne_i = y_i - \\hat{y}_i\n\nThen, we find the standard error of the estimate, s_e,\n\ns_e = \\sqrt{\\frac{\\sum_{i=1}^n e_i^2}{n-2}}\n\nFinally, we can find the standard error of \\hat{\\beta}_1, s_{\\hat{\\beta}_1},\n\ns_{\\hat{\\beta}_1} = \\frac{s_e}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}} = \\frac{s_e}{s_x \\sqrt{n-1}}"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#hypothesis-testing-for-beta_1-1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#hypothesis-testing-for-beta_1-1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Hypothesis Testing for \\beta_1",
    "text": "Hypothesis Testing for \\beta_1\n\nHypotheses\n\nH_0: \\ \\beta_1 = 0\nH_1: \\ \\beta_1 \\ne 0\n\nTest Statistic\n\nt_0 = \\frac{\\hat{\\beta}_1}{s_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1}{\\text{SE of }\\hat{\\beta}_1}\n\np-Value\n\np = 2 \\times P[t_{n-2} \\ge |t_0|]\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-8",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-8",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nFrom the golf example,\n\n\nsummary(m)\n\n\nCall:\nlm(formula = distance ~ speed, data = golf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8136 -2.0195  0.3542  2.0619  3.6881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -55.7966    48.3713  -1.154  0.29257    \nspeed         3.1661     0.4747   6.670  0.00055 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.883 on 6 degrees of freedom\nMultiple R-squared:  0.8811,    Adjusted R-squared:  0.8613 \nF-statistic: 44.48 on 1 and 6 DF,  p-value: 0.0005498\n\n\n\nt_0 for speed is 6.67\n\nSlope for speed is 3.1661 and the SE of the slope for speed is 0.4747\nWe can see that 3.1661/0.4747 = 6.67\n\nThe corresponding p-value is p = 0.00055, which we will represent as p &lt; 0.001."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-9",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-9",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nHypotheses\n\nH_0: \\ \\beta_{\\text{speed}} = 0\nH_1: \\ \\beta_{\\text{speed}} \\ne 0\n\nTest Statistic\n\nt_0 = 6.67\n\np-Value\n\np &lt; 0.001\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha; \\alpha=0.05"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#confidence-interval-for-beta_1",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#confidence-interval-for-beta_1",
    "title": "Correlation and Simple Linear Regression",
    "section": "Confidence Interval for \\beta_1",
    "text": "Confidence Interval for \\beta_1\n\nRecall that a point estimate by itself does not tell us how good our estimation is.\nThus, we are also interested in finding the confidence interval for \\beta_1.\n(1-\\alpha)100\\% CI for \\beta_1:  \\hat{\\beta}_1 \\pm t_{\\alpha/2,n-2} s_{\\hat{\\beta}_1} where the SE of the slope, s_{\\hat{\\beta}_1}, is as defined earlier.\nThe R syntax also reuses the model results in the confint() function.\n\n\nconfint(m) # for 95% CI\nconfint(m, level = conf_level) # for other levels"
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#example-10",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#example-10",
    "title": "Correlation and Simple Linear Regression",
    "section": "Example",
    "text": "Example\n\nLet’s find the confidence intervals for the golf data,\n\n\nconfint(m) # 95% CI\n\n                  2.5 %    97.5 %\n(Intercept) -174.157039 62.563818\nspeed          2.004539  4.327664\n\nconfint(m, level = 0.90) # 90% CI\n\n                    5 %     95 %\n(Intercept) -149.790863 38.19764\nspeed          2.243664  4.08854\n\n\n\nThe 95% CI for \\beta_{\\text{speed}} is (2.00, 4.33).\nThe 90% CI for \\beta_{\\text{speed}} is (2.24, 4.09)."
  },
  {
    "objectID": "files/lectures/W08-L1-simple-linear-regression.html#wrap-up",
    "href": "files/lectures/W08-L1-simple-linear-regression.html#wrap-up",
    "title": "Correlation and Simple Linear Regression",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we discussed the modeling of continuous data.\nWe have discussed simple linear regression, which means we have a single predictor in the model.\nNext week, we will extend to multiple regression (more than one predictor).\nRegression is ANOVA; ANOVA is regression.\n\nKeep this in mind if you are asked to assess assumptions."
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#introduction",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#introduction",
    "title": "Wilcoxon Rank Sum",
    "section": "Introduction",
    "text": "Introduction\n\nWe last discussed assumptions on t-tests\n\nDependent / paired t-test: normality\nIndependent two-sample t-test: normality and variance\n\nIf we break the normality assumption, we must look to nonparametric methods."
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#introduction-1",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#introduction-1",
    "title": "Wilcoxon Rank Sum",
    "section": "Introduction",
    "text": "Introduction\n\nThe t-tests we have already learned are considered parametric methods.\n\nThere is a distributional assumption on the test.\n\nNonparametric methods do not have distributional assumptions.\n\nWe typically transform the data to their ranks and then perform calculations.\n\nWhy don’t we always use nonparametric methods?\n\nThey are often less efficient: a larger sample size is required to achieve the same probability of a Type I error.\nThey discard useful information :("
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#ranking-data",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#ranking-data",
    "title": "Wilcoxon Rank Sum",
    "section": "Ranking Data",
    "text": "Ranking Data\n\nIn the nonparametric tests we will be learning, the data will be ranked.\nLet us first consider a simple example, x: \\ 1, 7, 10, 2, 6, 8\nOur first step is to reorder the data:x: \\ 1, 2, 6, 7, 8, 10\nThen, we replace with the ranks:R: \\ 1, 2, 3, 4, 5, 6"
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#ranking-data-1",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#ranking-data-1",
    "title": "Wilcoxon Rank Sum",
    "section": "Ranking Data",
    "text": "Ranking Data\n\nWhat if all data values are not unique?\n\nWe will assign the average ranks.\n\nFor example, x: \\ 9, 8, 8, 0, 3, 4, 4, 8\nLet’s reorder:x: \\ 0, 3, 4, 4, 8, 8, 8, 9\nRank ignoring ties:R: \\ 1, 2, 3, 4, 5, 6, 7, 8\nNow, the final rank:R: \\ 1, 2, 3.5, 3.5, 6, 6, 6, 8"
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum",
    "title": "Wilcoxon Rank Sum",
    "section": "Wilcoxon Rank Sum",
    "text": "Wilcoxon Rank Sum\nHypotheses\n\nH_0: M_1 - M_2 = M_0 | H_0: M_1 - M_2 \\le M_0 | H_0: M_1 - M_2 \\ge M_0\nH_1: M_1 - M_2 \\ne M_0 | H_1: M_1 - M_2 &gt; M_0 | H_1: M_1 - M_2 &lt; M_0\n\nTest Statistic & p-Value\n\nT = \\sum R_{\\text{sample 1}} - \\frac{n_1(n_1+1)}{2}\np = (calculated by R :))\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha.\n\nConclusion/Interpretation\n\n[Reject or fail to reject] H_0.\nThere [is or is not] sufficient evidence to suggest [alternative hypothesis in words]."
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum-1",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum-1",
    "title": "Wilcoxon Rank Sum",
    "section": "Wilcoxon Rank Sum",
    "text": "Wilcoxon Rank Sum\n\nWe will use the wilcox.test() function to perform the test,\n\n\nwilcox.test(continuous_variable ~ grouping_variable,\n            data = dataset_name,\n            alternative = \"alternative\",\n            mu = hypothesized_value,\n            exact = FALSE)\n\n\nLike before, R will use the group that is “first” in the grouping variable.\n\n“First” is in terms of numeric or alphabetical."
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum-2",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum-2",
    "title": "Wilcoxon Rank Sum",
    "section": "Wilcoxon Rank Sum",
    "text": "Wilcoxon Rank Sum\n\nWhen exposed to an infection, a person typically develops antibodies. The extent to which the antibodies respond can be measured by looking at a person’s titer, which is a measure of the number of antibodies present. The higher the titer is, the more antibodies that are present.\nThe following data represent the titers of 11 ill people and 11 healthy people exposed to the tularemia virus in Vermont.\nIs the level of titer in the ill group greater than the level of titer in the healthy group? Use the \\alpha = 0.10 level of significance.\n\n\ntiter_levels &lt;- tibble(level = c(640, 160, 1280, 320, 80, 640, 640, 160, 1280, 640, 160, \n                                  10, 320, 160, 160, 320, 320, 10, 320, 320, 80, 640),\n                       group = c(rep(\"ill\",11), rep(\"healthy\",11)))\n\n\nRecall the R syntax,\n\n\nwilcox.test(continuous_variable ~ grouping_variable,\n            data = dataset_name,\n            alternative = \"alternative\",\n            mu = hypothesized_value,\n            exact = FALSE)"
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum-3",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum-3",
    "title": "Wilcoxon Rank Sum",
    "section": "Wilcoxon Rank Sum",
    "text": "Wilcoxon Rank Sum\n\nIs the level of titer in the ill group greater than the level of titer in the healthy group?\n\n\nwilcox.test(level ~ group, \n            data = titer_levels,\n            alternative = \"less\",\n            exact = FALSE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  level by group\nW = 35, p-value = 0.04657\nalternative hypothesis: true location shift is less than 0"
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum-4",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wilcoxon-rank-sum-4",
    "title": "Wilcoxon Rank Sum",
    "section": "Wilcoxon Rank Sum",
    "text": "Wilcoxon Rank Sum\nHypotheses\n\nH_0: \\ M_{\\text{ill}} \\le M_{\\text{healthy}}\nH_1: \\ M_{\\text{ill}} &gt; M_{\\text{healthy}}\n\nTest Statistic and p-Value\n\nW_0 = 35\np = 0.047\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = 0.10.\n\nConclusion/Interpretation\n\nReject H_0.\nThere is sufficient evidence to suggest that the level of titer in the ill group is greater than the level of titer in the healthy group."
  },
  {
    "objectID": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wrap-up",
    "href": "files/lectures/W04-L3-Wilcoxon-rank-sum.html#wrap-up",
    "title": "Wilcoxon Rank Sum",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we reviewed the Wilcoxon rank sum test.\n\nNonparametric alternative to the two-sample t-test.\n\nNext lecture:\n\nWilcoxon signed rank"
  },
  {
    "objectID": "files/assignments/Assignment2.html",
    "href": "files/assignments/Assignment2.html",
    "title": "Assignment 2 - Spring 2025",
    "section": "",
    "text": "## DO NOT EDIT THIS SECTION\nlibrary(tidyverse)\nlibrary(gsheet)\ndata &lt;- as_tibble(gsheet2tbl(\"https://docs.google.com/spreadsheets/d/1H3TP-2SBMGleriJLESOe1cdCjtSj2F76bUh5iBqC8tI/edit#gid=2142000894\")) %&gt;%\n  mutate(PA3cat = as.factor(PA3cat),\n         BMI3cat = as.factor(BMI3cat),\n         everSmoker = as.factor(everSmoker)) %&gt;%\n  select(subjid, age, weight, BMI, BMI3cat, PA3cat, sbp, everSmoker) %&gt;%\n  na.omit()\n## DO NOT EDIT THIS SECTION\n\n1. Consider a sample of data from the Jackson Heart Study. In this problem, we will be examining body mass index (BMI; kg/m2) as a function of health as categorized by physical activity (PA3cat: Ideal Health, Intermediate Health, and ).\na. Find the appropriate summary statistics to fill in the table below; note that we are creating columns for each category of PA3cat. Continuous variables should be described using the mean and standard deviation while categorical variables should be described using the count and column percentage.\n\n\n\n\n\n\n\n\n\n\nIdeal Health (n = ?)\nIntermediate Health (n = ?)\nPoor Health (n = ?)\n\n\n\n\nAge\nmean (sd)\n\n\n\n\nWeight\nmean (sd)\n\n\n\n\nBMI\nmean (sd)\n\n\n\n\nSBP\nmean (sd)\n\n\n\n\nCurrent or Former Smoker\nn (column %)\n\n\n\n\n\n\nb. Use ANOVA to determine if there is a difference in BMI (BMI) between the three levels of health as categorized by physical activity (PA3cat). Test at the \\(\\alpha=0.01\\) level. Remember to typeset your hypothesis test.\nc. Use the Kruskal-Wallis to determine if there is a difference in BMI (BMI) between the three levels of health as categorized by physical activity (PA3cat). Test at the \\(\\alpha=0.01\\) level. Remember to typeset your hypothesis test.\nd. State, explain, and assess the ANOVA assumptions. Remember to state your conclusion (that you either meet the ANOVA assumptions or you do not).\ne. Based on your responses in part (d), which test’s result are you going to present to the lead scientist at the JHS? (Hint: you will pick either the analysis (b) or (c)).\nf. Perform the appropriate posthoc test to determine pairwise differences between the three groups. Test at the \\(\\alpha=0.01\\) level. Make sure that you state the resulting pairwise differences.\ng. Construct a(ny) graph to help explain the results from the analysis chosen in part (f).\nh. Write a brief paragraph describing the results of your analysis. Include summary statistics, results of your chosen hypothesis test from part (f), and the graph from part (g).\n2. Again, consider the sample of data from the JHS. In this problem, we will be examining systolic blood pressure (sbp; mmHg) as a function of health as categorized by body mass index (BMI3cat), smoking status (everSmoker), and the interaction between health as categorized by body mass index and smoking status.\na. Find the appropriate summary statistics to fill in the table below; note that we are creating columns for each category of BMI3cat. Continuous variables should be described using the mean and standard deviation while categorical variables should be described using the count and column percentage.\n\n\n\n\n\n\n\n\n\n\nIdeal Health (n = ?)\nIntermediate Health (n = ?)\nPoor Health (n = ?)\n\n\n\n\nAge\n\n\n\n\n\nWeight\n\n\n\n\n\nSBP\n\n\n\n\n\nCurrent or Former Smoker\n\n\n\n\n\n\n\nb. Use ANOVA to determine if there is a difference in systolic blood pressure between health as categorized by body mass index, smoking status, and the interaction between health as categorized by body mass index and smoking status. Test at the \\(\\alpha=0.01\\) level. Remember to remove the interaction term if appropriate to do so.\nc. Construct a profile plot to help explain results of part (b).\nd. Use the appropriate posthoc test to determine differences in main effects or the interaction term, whichever is appropriate for the results in part (b). Test at the \\(\\alpha=0.01\\) level. Make sure that you state the resulting pairwise differences.\ne. Write a brief paragraph describing the results of your analysis. Include summary statistics, results of the hypothesis testing in part (b), results of the posthoc test in part (d), and the profile plot in part (c) to help convey the results."
  },
  {
    "objectID": "files/assignments/Assignment1.html",
    "href": "files/assignments/Assignment1.html",
    "title": "Assignment 1 - Spring 2025",
    "section": "",
    "text": "1. Describe the data by completing the following table:\n\n\n\n\n\n\n\n\n\n\n\n2000\n2015\n\n\n\n\nSouth\nMean (Standard Deviation)\n132076.1 (109494.7)\nINSERT FOR 2015\n\n\nWest\nMean (Standard Deviation)\nINSERT FOR 2000\nINSERT FOR 2015\n\n\n\n\n\ndata %&gt;% # pipe in data\n  group_by(Region) %&gt;% # tell R we want this by \"Region\"\n  summarize(mean00 = mean(y2000), # request mean for 2000\n            sd00 = sd(y2000)) # request standard deviation for 2000\n\n# A tibble: 4 × 3\n  Region     mean00    sd00\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n1 Midwest   204973. 184965.\n2 Northeast 225918. 308950.\n3 South     132076. 109495.\n4 West      153955. 255145.\n\n\n2. Construct side-by-side boxplots comparing the West and Midwest regions for 2000 and 2015.\n\n\n\nbp00 &lt;- data_south_west %&gt;%\n  ggplot(aes(y= y2000, x=Region, fill=Region)) +\n          geom_boxplot() +\n          theme_minimal() +\n          labs(y = \"Spending\", x = \"Region\") +\n          ylim(min=0, max=1400000) +\n          ggtitle(\"2000\")\n\nbp00 &lt;- data_south_west %&gt;%\n  ggplot(aes(y= y2000, x=Region, fill=Region)) +\n          geom_boxplot() +\n          theme_minimal() +\n          labs(y = \"Spending\", x = \"Region\") +\n          ylim(min=0, max=1400000) +\n          ggtitle(\"2000\")\n\nggarrange(bp00, bp15, ncol=2)\n\nError: object 'bp15' not found\n\n\n3. Describe the distributions. Are they skewed? Do you think there’s a difference between the two regions?\n4. Use the appropriate t-test to determine if there is a difference in library spending between the South and West regions in 2000. Test at the \\(\\alpha=0.05\\) level. Remember to typeset the results.\n\nt.test(y2000 ~ Region,\n       data = data_south_west,\n       mu = 0,\n       alternative = \"two\")\n\n\n    Welch Two Sample t-test\n\ndata:  y2000 by Region\nt = -0.29898, df = 16.926, p-value = 0.7686\nalternative hypothesis: true difference in means between group South and group West is not equal to 0\n95 percent confidence interval:\n -176325.0  132566.6\nsample estimates:\nmean in group South  mean in group West \n           132076.1            153955.3 \n\n\nHypotheses\n\n\\(H_0:\\) \\(\\mu_s-\\mu_w = 0\\)\n\\(H_1:\\) \\(\\mu_s-\\mu_w \\ne 0\\)\n\nTest Statistic and p-Value\n\n\\(t_0 = -0.30\\)\n\\(p = 0.7686\\)\n\nRejection Region\n\nReject \\(H_0\\) if \\(p &lt; \\alpha\\); \\(\\alpha=0.05\\).\n\nConclusion/Interpretation\n\nFail to reject \\(H_0\\).\nThere is not sufficient evidence to suggest that the spending in the south and west regions is different.\n\n5. Graphically assess the normality assumption associated with the t-test in Q4. Remember to state whether or not the assumption is broken.\n6. Use the appropriate nonparametric test to determine if there is a difference in library spending between the South and West regions in 2000. Test at the \\(\\alpha=0.05\\) level. Remember to typeset the results.\n7. Based on your answer to Q5, which test should be reported? Explain why you’re choosing the test you’re choosing. (Hint: you will either choose the t-test or the nonparametric test.)\n8. Use the appropriate t-test to determine if there is a difference in library spending between 2000 and 2015 for all states. Test at the \\(\\alpha=0.05\\) level. Remember to typeset the results.\n9. Assess the normality assumption associated with the t-test in Q8. Remember to state whether or not the assumption is broken.\n10. Use the appropriate nonparametric test to determine if there is a difference in library spending between 2000 and 2015 for all states. Test at the \\(\\alpha=0.05\\) level. Remember to typeset the results.\n11. Based on your answer to Q9, which test should be reported? Explain why you’re choosing the test you’re choosing. (Hint: you will either choose the t-test or the nonparametric test.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "<b>STA4173 - Biostatistics - Spring 2025</b>",
    "section": "",
    "text": "Tentative Schedule\n\n\n\n\n\n\n\n\nDay\nTopic\nSlides\n\n\n\n\nR 01/09\nReview of estimation\n \n\n\nT 01/14\nIndependent t test\n \n\n\nSnow & Illness\ndependent t-test\n \n\n\n\nt-test assumptions\n \n\n\n\nWilcoxon rank sum (independent)\n \n\n\n\nWilcoxon signed rank (dependent)\n \n\n\nT 02/04\nAssignment 1\n\n\n\nR 02/06\nAssignment 1\n\n\n\nT 02/11\nProject: planning period\n\n\n\nR 02/13\nOne-way ANOVA and posthoc\n \n\n\nT 02/18\nANOVA assumptions & Kruskal-Wallis\n \n\n\nR 02/20\nTwo-Way ANOVA\n \n\n\nT 02/25\nSimple linear regression and correlation\n \n\n\nR 02/27\nProject: planning meeting\n\n\n\nT 03/04\nMultiple regression\n \n\n\nR 03/06\nCategorical analysis\n\n\n\nT 03/11\nLogistic regression\n\n\n\nR 03/13\nProject: planning\n\n\n\nT 03/18\nSpring Break!\n\n\n\nR 03/20\nSpring Break!\n\n\n\nT 03/25\nProject: analysis\n\n\n\nR 03/27\nOUR: What is a Poster?\n\n\n\nT 04/01\nProject: analysis\n\n\n\nR 04/03\nProject: summary\n\n\n\nT 04/08\nProject: poster draft\n\n\n\nR 04/10\nIndividual group meetings with Dr. Seals\n\n\n\nT 04/15\nPoster presentation practice\n\n\n\nR 04/17\nOUR Symposium\n\n\n\nT 04/22\nCatch up!\n\n\n\nR 04/24\nProject: reflections"
  },
  {
    "objectID": "files/assignments/Assignment0.html",
    "href": "files/assignments/Assignment0.html",
    "title": "Introductions",
    "section": "",
    "text": "What is your name as it appears on my roster in Classmate? (We are using this assignment for attendance verification.)\nHow is your name pronounced?\nIs there another name you prefer to go by?\nWhat are your pronouns?\nWhat is your major(s) and minor(s)?\nWhat statistics courses have you taken previously?\nAre you familiar with statistical programming? (e.g., R, python, SAS, etc.)\nDo you have any questions or concerns about this course? If so, please list them and we will respond when we grade this assignment.\nWhat interests do you have, outside of school and work?\nWhat are three of your favorite things?\nIf you were independently wealthy, what would you do with your time?"
  },
  {
    "objectID": "files/assignments/Assignment3.html",
    "href": "files/assignments/Assignment3.html",
    "title": "Assignment 3 - Spring 2025",
    "section": "",
    "text": "A regional airline transfers passengers from small airports to a larger regional hub airport. The airline’s data analyst was assigned to estimate the revenue (in thousands of dollars) generated by each of the 22 small airports based on two variables: the distance from each airport (in miles) to the hub and the population (in hundreds) of the cities in which each of the 22 airports is located. The data are given here: Google Sheet.\n\n# DO NOT EDIT THIS CHUNK\nlibrary(tidyverse)\nlibrary(gsheet)\nairline_data &lt;- gsheet2tbl('https://docs.google.com/spreadsheets/d/10ZN0jMYdVsG2ucIJFSTquVRW916Jj-JmZIGQuqSQhy0/edit?usp=sharing')\n# DO NOT EDIT THIS CHUNK\n\n1. Model the revenue as a function of distance and population. Remember to state the resulting model.\n2. Provide brief interpretations for the slopes in the model.\n\n2a: distance -\n2b: population -\n\n3. Use the appropriate hypothesis test to determine if distance is a significant predictor of revenue. Test at the \\(\\alpha=0.05\\) level. Remember to typeset your results.\n4. Use the appropriate hypothesis test to determine if population is a significant predictor of revenue. Test at the \\(\\alpha=0.05\\) level. Remember to typeset your results.\n5. Use the appropriate hypothesis test to determine if this is a significant regression line. Test at the \\(\\alpha=0.05\\) level.\n6. How well does the data fit the line? Find the adjusted \\(R^2\\) value and comment on it.\n7. How many observations are considered outliers?\n8. Create a second dataset that contains observations that are not classified as outliers.\n\nairline_no_outliers &lt;- airline_data %&gt;%\n  filter(abs(rstandard(model_name))&gt;2.5)\n\nError in `filter()`:\nℹ In argument: `abs(rstandard(model_name)) &gt; 2.5`.\nCaused by error:\n! object 'model_name' not found\n\n\n\n9. Using the second dataset, model the revenue as a function of distance and population. Remember to state the resulting model.\n10. Provide brief interpretations for the slopes in the model.\n\n10a: distance -\n10b: population -\n\n11. How different are the interpretations from those in Q2?\n12. Use the appropriate hypothesis test to determine if distance is a significant predictor of revenue. Test at the \\(\\alpha=0.05\\) level. Remember to typeset your results.\n13. Use the appropriate hypothesis test to determine if population is a significant predictor of revenue. Test at the \\(\\alpha=0.05\\) level. Remember to typeset your results.\n14. How different are the results for Q12 and Q13 as compared to the full dataset’s analysis in Q3 and Q4?\n15. Use the appropriate hypothesis test to determine if this is a significant regression line. Test at the \\(\\alpha=0.05\\) level.\n16. How different are the results for Q15 as compared to the full dataset’s analysis in Q5?\n17. Check the model assumptions - is our model valid?\n18. Why do you think I chose this as your project? What are you taking away from the analyses you have done?"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-two-way-anova",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-two-way-anova",
    "title": "Two-Way ANOVA",
    "section": "Introduction: Two-Way ANOVA",
    "text": "Introduction: Two-Way ANOVA\n\nRecall that ANOVA allows us to compare the means of three or more groups.\nIn one-way ANOVA, we are only considering one factor (grouping variable).\nNow, we will discuss two-way ANOVA, which allows us to consider a second factor (grouping variable).\nWe now partition the SSTrt into the different factors under consideration.\nRecall that SSE is the “catch all” for unexplained variance.\n\nWhen we add factors to our model, we are moving part of the SSE into the SSTrt."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-two-way-anova-1",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-two-way-anova-1",
    "title": "Two-Way ANOVA",
    "section": "Introduction: Two-Way ANOVA",
    "text": "Introduction: Two-Way ANOVA\n\nLet’s discuss some of the language used in two-way ANOVA.\nFactor A has a levels.\n\nSmoking status: non-smoker and smoker\n\nFactor B has b levels.\n\nHypertension: normotensive and hypertensive\n\nThere are ab treatment groups.\n\nNon-smoker, normotensive\nNon-smoker, hypertensive\nSmoker, normotensive\nSmoker, hypertensive"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-two-way-anova-2",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-two-way-anova-2",
    "title": "Two-Way ANOVA",
    "section": "Introduction: Two-Way ANOVA",
    "text": "Introduction: Two-Way ANOVA\n\nNow that we are including two factors, we must consider the interaction between the two.\n\nInteraction: The relationship between [outcome] and [factor 1] depends on the level of [factor 2].\n\nIn our example, suppose we are looking at weight as the outcome. If an interaction is detected,\n\nthe relationship between weight and hypertension status depends on smoking status, OR\nthe relationship between weight and smoking status depends on hypertension status."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-two-way-anova-3",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-two-way-anova-3",
    "title": "Two-Way ANOVA",
    "section": "Introduction: Two-Way ANOVA",
    "text": "Introduction: Two-Way ANOVA\n\nThe ANOVA table that we are working to construct:\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares\ndf\nMean Square\nF\n\n\n\n\nA\nSSA\ndfA\nMSA\nFA\n\n\nB\nSSB\ndfB\nMSB\nFB\n\n\nAB\nSSAB\ndfAB\nMSAB\nFAB\n\n\nError\nSSE\ndfE\nMSE\n\n\n\nTotal\nSSTot\ndfTot"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#computation",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#computation",
    "title": "Two-Way ANOVA",
    "section": "Computation",
    "text": "Computation\n\nLet there be a levels of factor A and b levels of factor B.\n\ny_{ijk} is the observation on the k^{th} experimental unit receiving the i^{th} level of factor A and the j^{th} level of factor B\ny_{i.} is the sum for all observations at the i^{th} level of factor A,\ny_{.j} is the sum for all observations at the j^{th} level of factor B,\ny_{ij} is the sum for observations at the i^{th} level of factor A and the j^{th} level of factor B,\ny_{..} is the sum of all observations,\n(y^2)_{..} is the sum of the squared observations,\nn is the number in each group (of a \\times b treatments)"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#computation-1",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#computation-1",
    "title": "Two-Way ANOVA",
    "section": "Computation",
    "text": "Computation\n\nTo find the SS and df: \n\\begin{align*}\n\\text{SS}_{\\text{A}} &= \\frac{\\sum_i y_{i.}^2}{bn}  - \\frac{(y_{..})^2}{abn} & \\text{df}_{\\text{A}} &= a-1 \\\\\n\\text{SS}_{\\text{B}} &= \\frac{\\sum_j y_{.j}^2}{an} - \\frac{(y_{..})^2}{abn} & \\text{df}_{\\text{B}} &= b-1 \\\\\n\\text{SS}_{\\text{AB}}&= \\frac{\\sum_{ij} y_{ij}^2}{n} - \\frac{(y_{..})^2}{abn} - \\text{SS}_{\\text{A}} - \\text{SS}_{\\text{B}} & \\text{df}_{\\text{AB}} &= (a-1)(b-1) \\\\\n\\text{SS}_{\\text{E}} &= \\text{SS}_{\\text{Tot}} - \\text{SS}_{\\text{A}} - \\text{SS}_{\\text{B}} - \\text{SS}_{\\text{AB}} & \\text{df}_{\\text{E}} &= ab(n-1)  \\\\\n\\text{SS}_{\\text{Tot}} &= (y^2)_{..} - \\frac{(y_{..})^2}{abn} & \\text{df}_{\\text{Tot}} &= abn-1\n\\end{align*}"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#computation-2",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#computation-2",
    "title": "Two-Way ANOVA",
    "section": "Computation",
    "text": "Computation\n\nTo find the MS:\n\n\\text{MS}_{\\text{X}} = \\frac{\\text{SS}_{\\text{X}}}{\\text{df}_{\\text{X}}}\n\nTo find the test statistic:\n\n\\text{F}_{\\text{X}} = \\frac{\\text{MS}_{\\text{X}}}{\\text{MS}_{\\text{E}}}"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#r-syntax",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#r-syntax",
    "title": "Two-Way ANOVA",
    "section": "R Syntax",
    "text": "R Syntax\n\nWe will again use lm() and anova() OR aov() and summary() to analyze the data.\n\nFor reasons covered in one-way ANOVA, we will focus on using aov().\n\nAs we are specifying our model, we will:\n\ninclude additional terms with +\ninclude interaction terms with :\n\n\n\nm &lt;- aov(continuous_variable ~ factor_A + factor_B + factor_A:factor_B,\n         data = dataset_name)\nsummary(m)"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nSuppose we want to examine the effect of diet and age of mothers on the average birth weight (in ounces). Consider the following data,\n\n\n\n\n\nWhat is the outcome?\nWhat are the two factors?\nWhat is the interaction term?"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-1",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-1",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nlibrary(tidyverse)\nexample &lt;- tibble(birthweight = c(157.78, 136.79, 138.84, # age 20-29, diet 1\n                                  139.72, 125.47, 117.14, # age 20-29, diet 2\n                                  129.35, 110.73, 118.38, # age 20-29, diet 3\n                                  137.07, 146.28, 130.27, # age 30-39, diet 1\n                                  117.46, 128.54, 99.16, # age 30-39, diet 2\n                                  97.43, 125.26, 115.42), # age 30-39, diet 3\n                  diet = as.factor(c(1, 1, 1, \n                                     2, 2, 2, \n                                     3, 3, 3, \n                                     1, 1, 1, \n                                     2, 2, 2, \n                                     3, 3, 3)),\n                  age = as.factor(c(20, 20, 20, \n                                    20, 20, 20, \n                                    20, 20, 20, \n                                    30, 30, 30, \n                                    30, 30, 30, \n                                    30, 30, 30)))"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-2",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-2",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nLet’s find the ANOVA table,\n\n\nm &lt;- aov(birthweight ~ diet + age + diet:age, data = example)\nsummary(m)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndiet         2 2104.7  1052.3   7.555 0.00752 **\nage          1  332.0   332.0   2.384 0.14855   \ndiet:age     2   32.5    16.3   0.117 0.89083   \nResiduals   12 1671.5   139.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#testing-interactions",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#testing-interactions",
    "title": "Two-Way ANOVA",
    "section": "Testing Interactions",
    "text": "Testing Interactions\n\nHypotheses\n\nH_0: there is not an interaction between [factor A] and [factor B]\nH_1: there is an interaction between [factor A] and [factor B]\n\nTest Statistic and p-Value\n\nF_{\\text{AB}} (pull from ANOVA table)\np = P[F_{\\text{df}_{\\text{AB}}, \\text{df}_{\\text{E}}} \\ge F_{\\text{AB}}]\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-3",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-3",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nLet’s now test the interaction between age and diet. Test at the \\alpha=0.05 level.\nHere’s the information we need:\n\n\nsummary(m)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndiet         2 2104.7  1052.3   7.555 0.00752 **\nage          1  332.0   332.0   2.384 0.14855   \ndiet:age     2   32.5    16.3   0.117 0.89083   \nResiduals   12 1671.5   139.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-4",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-4",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nHypotheses\n\nH_0: there is not an interaction between age and diet\nH_1: there is an interaction between age and diet\n\nTest Statistic\n\nF_{\\text{AB}} = 0.117\np = 0.891\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha; \\alpha=0.05.\n\nConclusion/Interpretation\n\nFail to reject H_0. There is not sufficient evidence to suggest that the relationship between average birth weight and age depends on the mother’s diet."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#testing-interactions-1",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#testing-interactions-1",
    "title": "Two-Way ANOVA",
    "section": "Testing Interactions",
    "text": "Testing Interactions\n\nWhat happens after testing for an interaction?\n\nIf significant (reject H_0), we can:\n\nConstruct a profile plot to visualize what’s going on and help explain the effect.\nExamine posthoc testing to further examine the interaction.\n\nIf not significant (FTR H_0), we should remove the interaction term so that we can test and interpret the main effects.\n\nRemember that we cannot look at main effects (A and B alone) when the interaction is included in the model!"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#removing-interactions",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#removing-interactions",
    "title": "Two-Way ANOVA",
    "section": "Removing Interactions",
    "text": "Removing Interactions\n\nIf the interaction term is not significant, it should be removed from the ANOVA table so that we can test and interpret the main effects.\nHow to remove an interaction:\n\nRewrite ANOVA table without interaction; do not change SS, df, and MS for the main effects and total.\nUpdate the error term: add the SS_{\\text{AB}} to SS_{\\text{E}} and df_{\\text{AB}} to df_{\\text{E}}.\nRecalculate MS_{\\text{E}}.\nRecalculate F statistics for the main effects and perform appropriate hypothesis tests.\n\nIn R, we just literally remove the interaction term.\n\n\nm &lt;- aov(continuous_variable ~ factor_A + factor_B,\n         data = dataset_name)\nsummary(m)"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-5",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-5",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nLet’s now remove the interaction between diet and maternal age.\n\n\n#m &lt;- aov(birthweight ~ diet + age + diet:age, data = example)\nm &lt;- aov(birthweight ~ diet + age, data = example)\nsummary(m)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndiet         2   2105  1052.3   8.646 0.00359 **\nage          1    332   332.0   2.728 0.12085   \nResiduals   14   1704   121.7                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#testing-main-effects",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#testing-main-effects",
    "title": "Two-Way ANOVA",
    "section": "Testing Main Effects",
    "text": "Testing Main Effects\n\nNow that we have removed the interaction term, we can test for main effects.\nHypotheses\n\nH_0: there is not a main effect of [factor X]\nH_1: there not a main effect of [factor X]\n\nTest Statistic and p-Value\n\nF_{\\text{X}} (pull from ANOVA table)\np = P[F_{\\text{df}_{\\text{X}}, \\text{df}_{\\text{E}}} \\ge F_{\\text{X}}]\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#testing-main-effects-1",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#testing-main-effects-1",
    "title": "Two-Way ANOVA",
    "section": "Testing Main Effects",
    "text": "Testing Main Effects\n\nA note on hypotheses – we are writing them in sentence form here, however, we can write them mathematically.\nFor Factor A with a levels,\n\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_a\nH_1: at least one is different\n\nFor Factor B with b levels,\n\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_b\nH_1: at least one is different"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-6",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-6",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nWe now want to determine if there are main effects of age and diet. Test at the \\alpha=0.05 level.\nThe ANOVA table without the interaction term:\n\n\nsummary(m)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ndiet         2   2105  1052.3   8.646 0.00359 **\nage          1    332   332.0   2.728 0.12085   \nResiduals   14   1704   121.7                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-7",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-7",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nHypotheses\n\nH_0: there is not a main effect of diet (\\mu_1 = \\mu_2 = \\mu_3)\nH_1: there is a main effect of diet (at least one \\mu_i is different)\n\nTest Statistic and p-Value\n\nF_{\\text{diet}} = 8.646\np = 0.004\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha; \\alpha=0.05.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that at least one diet reults in a different birth weight."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-8",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-8",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nHypotheses\n\nH_0: there is not a main effect of age (\\mu_1 = \\mu_2)\nH_1: there is a main effect of age (\\mu_1 \\ne \\mu_2)\n\nTest Statistic and p-Value\n\nF_{\\text{age}} = 2.728\np = 0.121\n\nRejection Region\n\nReject H_0 if p&lt;\\alpha; \\alpha=0.05.\n\nConclusion/Interpretation\n\nFail to reject H_0. There is not sufficient evidence to suggest that there is a difference in birth weights across the age groups."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-beyond-the-f-test",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#introduction-beyond-the-f-test",
    "title": "Two-Way ANOVA",
    "section": "Introduction: Beyond the F Test",
    "text": "Introduction: Beyond the F Test\n\nIn this lecture, we are discussing two-way ANOVA.\nSo far, we have learned how to:\n\nCreate an ANOVA model with an interaction term\nTest the interaction term\nCreate an ANOVA model without an interaction term\nTest the main effects\n\nNow we will learn how to best communicate results, whether it’s the resulting interaction term or main effects."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#profile-plots",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#profile-plots",
    "title": "Two-Way ANOVA",
    "section": "Profile Plots",
    "text": "Profile Plots\n\nIf we detect an interaction term, we must give meaning to it.\nAn easy way to do this is to plot the treatment group means to visualize the interaction. This is called a profile plot.\n\ny-axis: always the average outcome\nx-axis: either factor A or B\n\nif only one factor is ordinal, use it for the x-axis\nif there are two ordinal or two nominal factors, select the factor with the largest number of levels for the x-axis\n\nlines on the plot: the factor that was not selected for the x-axis\n\nNote that this is just a graph of the means – it’s valid to construct a profile plot even if the interaction is not sigificant."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-9",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-9",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nLet’s first find the treatment means for the birth weight data,\n\n\nmeans &lt;- example %&gt;%\n  group_by(diet, age) %&gt;%\n  summarize(mean = mean(birthweight)) %&gt;%\n  ungroup()\nhead(means)"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-10",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-10",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nThe average birth weight will go on our y-axis.\nBecause only maternal age is ordinal, it will go on our x-axis.\nThus, the lines will represent the diet\n\nThis means I want to restructure the dataset with the means to have a column for each of the diets\n\n\n\nA &lt;- means %&gt;% filter(diet == 1) %&gt;% rename(d1 = mean) %&gt;% select(-diet)\nB &lt;- means %&gt;% filter(diet == 2) %&gt;% rename(d2 = mean) %&gt;% select(-diet)\nC &lt;- means %&gt;% filter(diet == 3) %&gt;% rename(d3 = mean) %&gt;% select(-diet)\ngraph &lt;- full_join(A, B, by = \"age\")\ngraph &lt;- full_join(graph, C, by = \"age\")"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-11",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-11",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\ngraph"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-12",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-12",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nBuilding profile plots using ggplot() is a process.\n\nFirst, include a geom_line() for each level of the factor creating the lines.\n\n\n\ngraph %&gt;% \n  ggplot(aes(x = age, group = 1)) +\n  geom_line(aes(y = d1), color = \"pink\") + # diet 1\n  geom_line(aes(y = d2), color = \"purple\") + # diet 2\n  geom_line(aes(y = d3), color = \"blue\") + # diet 3\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-13",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-13",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nBuilding profile plots using ggplot() is a process.\n\nFirst, include a geom_line() for each level of the factor creating the lines."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-14",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-14",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nBuilding profile plots using ggplot() is a process.\n\nThen, add geom_text() to label each line (use the line colors to make sure everything is labeled properly).\n\n\n\ngraph %&gt;% \n  ggplot(aes(x = age, group = 1)) +\n  geom_line(aes(y = d1), color = \"pink\") + # diet 1\n  geom_line(aes(y = d2), color = \"purple\") + # diet 2\n  geom_line(aes(y = d3), color = \"blue\") + # diet 3 \n  geom_text(aes(x = \"30\" , y = 137, label = \"Diet 1\"), color = \"pink\")  + # diet 1\n  geom_text(aes(x = \"30\" , y = 115, label = \"Diet 2\"), color = \"purple\")  + # diet 2\n  geom_text(aes(x = \"30\" , y = 110, label = \"Diet 3\"), color = \"blue\")  + # diet 3\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-15",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-15",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nBuilding profile plots using ggplot() is a process.\n\nThen, add geom_text() to label each line (use the line colors to make sure everything is labeled properly)."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-16",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-16",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nBuilding profile plots using ggplot() is a process.\n\nThen, clean up time: fix axis titles and change line colors to black*\n\n\n\ngraph %&gt;% \n  ggplot(aes(x = age, group = 1)) +\n  geom_line(aes(y = d1), color = \"black\") + # diet 1\n  geom_line(aes(y = d2), color = \"black\") + # diet 2\n  geom_line(aes(y = d3), color = \"black\") + # diet 3+ \n  geom_text(aes(x = \"30\" , y = 137, label = \"Diet 1\"), color = \"black\")  + # diet 1\n  geom_text(aes(x = \"30\" , y = 115, label = \"Diet 2\"), color = \"black\")  + # diet 2\n  geom_text(aes(x = \"30\" , y = 110, label = \"Diet 3\"), color = \"black\")  + # diet 3\n  labs(x = \"Maternal Age\",\n       y = \"Average Birth Weight\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-17",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-17",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#posthoc-tests",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#posthoc-tests",
    "title": "Two-Way ANOVA",
    "section": "Posthoc Tests",
    "text": "Posthoc Tests\n\nWe can also apply posthoc testing to two-way ANOVA.\n\nIf the interaction is significant, we can compare all treatment groups.\nIf the interaction is not significant, we can look at the main effects.\n\nFor simplicity, we will only look at Tukey’s and Fisher’s.\nWhat is the difference between Tukey’s and Fisher’s?"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-18",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-18",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nLet’s apply Tukey’s to the model with the interaction term.\n\nImportant!! Remember that we are only doing this for illustration purposes - we should not perform posthoc testing on something non-significant.\n\n\n\nm &lt;- aov(birthweight ~ diet + age + diet:age, data = example)\nTukeyHSD(m)$\"diet:age\"\n\n                diff       lwr       upr      p adj\n2:20-1:20 -17.026667 -49.39499 15.341659 0.51846118\n3:20-1:20 -24.983333 -57.35166  7.384992 0.17257865\n1:30-1:20  -6.596667 -38.96499 25.771659 0.98037295\n2:30-1:20 -29.416667 -61.78499  2.951659 0.08309462\n3:30-1:20 -31.766667 -64.13499  0.601659 0.05550164\n3:20-2:20  -7.956667 -40.32499 24.411659 0.95688090\n1:30-2:20  10.430000 -21.93833 42.798326 0.87931551\n2:30-2:20 -12.390000 -44.75833 19.978326 0.78717713\n3:30-2:20 -14.740000 -47.10833 17.628326 0.65386678\n1:30-3:20  18.386667 -13.98166 50.754992 0.44210449\n2:30-3:20  -4.433333 -36.80166 27.934992 0.99674874\n3:30-3:20  -6.783333 -39.15166 25.584992 0.97786517\n2:30-1:30 -22.820000 -55.18833  9.548326 0.24087234\n3:30-1:30 -25.170000 -57.53833  7.198326 0.16754036\n3:30-2:30  -2.350000 -34.71833 30.018326 0.99984711"
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#example-19",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#example-19",
    "title": "Two-Way ANOVA",
    "section": "Example",
    "text": "Example\n\nLet’s apply Tukey’s to the model without the interaction term.\n\n\nm &lt;- aov(birthweight ~ diet + age, data = example)\nTukeyHSD(m)$diet\n\n          diff       lwr       upr      p adj\n2-1 -19.923333 -36.59450 -3.252169 0.01902981\n3-1 -25.076667 -41.74783 -8.405502 0.00397944\n3-2  -5.153333 -21.82450 11.517831 0.70372399\n\n\n\nWhich diets are significantly different? Use \\alpha=0.05."
  },
  {
    "objectID": "files/lectures/W07-L2-two-way-ANOVA.html#wrap-up",
    "href": "files/lectures/W07-L2-two-way-ANOVA.html#wrap-up",
    "title": "Two-Way ANOVA",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nWe did not explicitly discuss assumptions, but they are the same as in one-way ANOVA.\nFrom here, ANOVA can be used to specify any model.\n\nWe stopped with two-way interactions, but you could have three-way interactions, four-way interactions, etc.\nAs you can see, interactions quickly complicate the interpretation, so I try to stay away from anything higher than a two-way interaction.\n\nOur next module is regression analysis, which is the same as ANOVA, but represents the results differently.\n\nWe can quantify the relationship between two variables using regression."
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#introduction",
    "href": "files/lectures/W01-L1-review-of-estimation.html#introduction",
    "title": "Review of Statistical Estimation",
    "section": "Introduction",
    "text": "Introduction\n\nIn this lecture, we will review estimation\n\nContinuous variables\n\nMean\nMedian\nPercentiles / quartiles\nVariance and standard deviation\nInterquartile range\n\nCategorical variables\n\nCount\nPercentage\n\n\nWe will also discuss exploring data graphically"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#r-introduction",
    "href": "files/lectures/W01-L1-review-of-estimation.html#r-introduction",
    "title": "Review of Statistical Estimation",
    "section": "R: Introduction",
    "text": "R: Introduction\n\nIn this course, we will review formulas, but we will use R for computational purposes\n\nRemember to refer to the lecture notes for specific code needed\nCode is also available on this course’s GitHub repository\n\nYou can install R and RStudio if you wish; both are free.\n\nWe have access to the Posit Workbench (“the server”) through HMCSE.\n\nI know that this is probably the first time you are seeing R (or any sort of programming).\n\nThat is why we have “R lab” time built in to our course.\nRemember that I am not looking for perfection, but instead for competency."
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#todays-data-palmer-penguins",
    "href": "files/lectures/W01-L1-review-of-estimation.html#todays-data-palmer-penguins",
    "title": "Review of Statistical Estimation",
    "section": "Today’s Data: Palmer Penguins",
    "text": "Today’s Data: Palmer Penguins\n\nToday we will be demonstrating the basics using the Palmer Penguins dataset, available through R.\n\n\npenguins &lt;- palmerpenguins::penguins"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#types-of-variables",
    "href": "files/lectures/W01-L1-review-of-estimation.html#types-of-variables",
    "title": "Review of Statistical Estimation",
    "section": "Types of Variables",
    "text": "Types of Variables\n\n\n\n\n\n\nContinuous Variables\n\n\nA continuous variable is a variable that can has an infinite set of possible values.\n\n\n\n\nBetween any two possible values, there are an infinite number of possible values.\nThese typically arise from measurement. (Height, weight, etc.)\n\n\n\n\n\n\n\nDiscrete Variables\n\n\nA discrete variable is a variable that can only take on a finite set of possible values.\n\n\n\n\nThe possible values can usually be listed.\nThese typically arise from categorizing (work vs. home) or counting."
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#types-of-variables-1",
    "href": "files/lectures/W01-L1-review-of-estimation.html#types-of-variables-1",
    "title": "Review of Statistical Estimation",
    "section": "Types of Variables",
    "text": "Types of Variables"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#types-of-continuous-variables",
    "href": "files/lectures/W01-L1-review-of-estimation.html#types-of-continuous-variables",
    "title": "Review of Statistical Estimation",
    "section": "Types of Continuous Variables",
    "text": "Types of Continuous Variables\n\n\n\n\n\n\nRatio Variables\n\n\nA ratio variable is a variable that has a meaningful zero point, allowing comparisons of magnitude.\n\n\n\n\nTrue zero point indicates the absence of the quantity being measured.\nAll arithmetic operations (addition, subtraction, multiplication, division) are meaningful.\n\n\n\n\n\n\n\nInterval Variables\n\n\nAn interval variable has an arbitrary zero point and differences between values are meaningful.\n\n\n\n\nThe zero point does not indicate a true absence.\nA 1 unit difference always represents the same amount."
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#types-of-discrete-variables",
    "href": "files/lectures/W01-L1-review-of-estimation.html#types-of-discrete-variables",
    "title": "Review of Statistical Estimation",
    "section": "Types of Discrete Variables",
    "text": "Types of Discrete Variables\n\n\n\n\n\n\nOrdinal Variables\n\n\nAn ordinal variable has a meaningful order of responses; the exact differences between responses are not necessarily equal.\n\n\n\n\nWe understand which value is “greater” or “less,” but not by how much.\nArithmetic is not meaningful.\n\n\n\n\n\n\n\nNominal Variables\n\n\nA nominal variable has is no intrinsic order among the categories.\n\n\n\n\nCategories are used merely as labels or names.\nNo arithmetic or ordering operations are meaningful."
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#measures-of-centrality-mean",
    "href": "files/lectures/W01-L1-review-of-estimation.html#measures-of-centrality-mean",
    "title": "Review of Statistical Estimation",
    "section": "Measures of Centrality: Mean",
    "text": "Measures of Centrality: Mean\n\n\n\n\n\n\nSample Mean\n\n\nThe sample mean provides a single number that can represent a “typical” or central value in your data.\n\n\\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n}\n\n\n\n\n\nR syntax:\n\n\ndataset_name %&gt;% summarize(mean(variable_name, na.rm = TRUE))"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#measures-of-centrality-mean-1",
    "href": "files/lectures/W01-L1-review-of-estimation.html#measures-of-centrality-mean-1",
    "title": "Review of Statistical Estimation",
    "section": "Measures of Centrality: Mean",
    "text": "Measures of Centrality: Mean\n\nLet’s find the average weight (body_mass_g) of the penguins.\n\n\npenguins %&gt;% summarize(mean(body_mass_g, na.rm = TRUE))\n\n\n  \n\n\n\n\nLet’s find the average flipper length (flipper_length_mm) of the penguins.\n\n\npenguins %&gt;% summarize(mean(flipper_length_mm, na.rm = TRUE))"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#measures-of-centrality-median",
    "href": "files/lectures/W01-L1-review-of-estimation.html#measures-of-centrality-median",
    "title": "Review of Statistical Estimation",
    "section": "Measures of Centrality: Median",
    "text": "Measures of Centrality: Median\n\n\n\n\n\n\nSample Median\n\n\nThe sample median is the midpoint of a distribution, the number such that half the observations are smaller and the other half are larger.\n\nIf n is odd, the median is the single middle value.\nIf n is even, the median is the average of the two middle values.\n\n\n\n\n\nR syntax:\n\n\ndataset_name %&gt;% summarize(median(variable_name, na.rm = TRUE))"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#measures-of-centrality-median-1",
    "href": "files/lectures/W01-L1-review-of-estimation.html#measures-of-centrality-median-1",
    "title": "Review of Statistical Estimation",
    "section": "Measures of Centrality: Median",
    "text": "Measures of Centrality: Median\n\nLet’s find the median weight (body_mass_g) of the penguins.\n\n\npenguins %&gt;% summarize(median(body_mass_g, na.rm = TRUE))\n\n\n  \n\n\n\n\nLet’s find the median flipper length (flipper_length_mm) of the penguins.\n\n\npenguins %&gt;% summarize(median(flipper_length_mm, na.rm = TRUE))"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#measures-of-spread-variance-and-standard-deviation",
    "href": "files/lectures/W01-L1-review-of-estimation.html#measures-of-spread-variance-and-standard-deviation",
    "title": "Review of Statistical Estimation",
    "section": "Measures of Spread: Variance and Standard Deviation",
    "text": "Measures of Spread: Variance and Standard Deviation\n\n\n\n\n\n\nSample Variance\n\n\nThe sample variance measures how “widely spread” the data points are around the mean.\ns^2 = \\frac{\\sum_{i=1}^n x_i^2 - \\frac{(\\sum_{i=1}^n x_i)^2}{n}}{n-1}\n\n\n\n\nWhen we have a mound-shaped and symmetric distribution, most observations will fall within 2 standard deviations of the mean.\nVariance results in units2, which typically does not make sense.\n\n\n\n\n\n\n\nSample Standard Deviation\n\n\nThe sample standard deviation also measures how “widely spread” the data points are around the mean.\ns = \\sqrt{s^2}\n\n\n\n\nStandard deviation is the square root of the variance, measuring spread in the original units of the data.\nR syntax:\n\n\ndataset_name %&gt;% summarize(var(variable_name, na.rm = TRUE), \n                           sd(variable_name, na.rm = TRUE))"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#measures-of-spread-variance-and-standard-deviation-1",
    "href": "files/lectures/W01-L1-review-of-estimation.html#measures-of-spread-variance-and-standard-deviation-1",
    "title": "Review of Statistical Estimation",
    "section": "Measures of Spread: Variance and Standard Deviation",
    "text": "Measures of Spread: Variance and Standard Deviation\n\nLet’s find the variance and standard deviation of the weight (body_mass_g) of the penguins.\n\n\npenguins %&gt;% summarize(var(body_mass_g, na.rm = TRUE),\n                       sd(body_mass_g, na.rm = TRUE))\n\n\n  \n\n\n\n\nLet’s find the variance and standard deviation of the flipper length (flipper_length_mm) of the penguins.\n\n\npenguins %&gt;% summarize(var(flipper_length_mm, na.rm = TRUE),\n                       sd(flipper_length_mm, na.rm = TRUE))"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#measures-of-spread-interquartile-range",
    "href": "files/lectures/W01-L1-review-of-estimation.html#measures-of-spread-interquartile-range",
    "title": "Review of Statistical Estimation",
    "section": "Measures of Spread: Interquartile Range",
    "text": "Measures of Spread: Interquartile Range\n\n\n\n\n\n\nSample Interquartile Range\n\n\nThe sample interquartile range measures the spread of the middle 50% of data.\n\\text{IQR} = P_{75}-P_{25}\n\n\n\n\nR syntax:\n\n\ndataset_name %&gt;% summarize(IQR(variable_name))"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#measures-of-spread-interquartile-range-1",
    "href": "files/lectures/W01-L1-review-of-estimation.html#measures-of-spread-interquartile-range-1",
    "title": "Review of Statistical Estimation",
    "section": "Measures of Spread: Interquartile Range",
    "text": "Measures of Spread: Interquartile Range\n\nLet’s find the IQR of the weight (body_mass_g) of the penguins.\n\n\npenguins %&gt;% summarize(IQR(body_mass_g, na.rm = TRUE))\n\n\n  \n\n\n\n\nLet’s find the IQR of the flipper length (flipper_length_mm) of the penguins.\n\n\npenguins %&gt;% summarize(IQR(flipper_length_mm, na.rm = TRUE))"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#mean-standard-deviation-vs.-median-iqr",
    "href": "files/lectures/W01-L1-review-of-estimation.html#mean-standard-deviation-vs.-median-iqr",
    "title": "Review of Statistical Estimation",
    "section": "Mean & Standard Deviation vs. Median & IQR",
    "text": "Mean & Standard Deviation vs. Median & IQR\n\nWhen should we use the mean vs. the median to describe the center of the distribution?\n\nMound-shaped and symmetric \\to \\bar{x} & s.\nNot mound-shaped and symmetric \\to M & \\text{IQR}.\n\n… How do we know the shape of the distribution?\nWe will explore histograms."
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms",
    "href": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms",
    "title": "Review of Statistical Estimation",
    "section": "Graphs: Histograms",
    "text": "Graphs: Histograms"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-1",
    "href": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-1",
    "title": "Review of Statistical Estimation",
    "section": "Graphs: Histograms",
    "text": "Graphs: Histograms"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-2",
    "href": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-2",
    "title": "Review of Statistical Estimation",
    "section": "Graphs: Histograms",
    "text": "Graphs: Histograms"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-3",
    "href": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-3",
    "title": "Review of Statistical Estimation",
    "section": "Graphs: Histograms",
    "text": "Graphs: Histograms"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-r-code",
    "href": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-r-code",
    "title": "Review of Statistical Estimation",
    "section": "Graphs: Histograms (R code)",
    "text": "Graphs: Histograms (R code)\n\nWe are using the ggplot2 package for graphing.\n\nIt will always start with ggplot().\nWe will then layer elements on top.\n\nR syntax:\n\n\ndataset_name %&gt;% \n  ggplot(aes(x=variable_name)) + \n  geom_histogram()"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-4",
    "href": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-4",
    "title": "Review of Statistical Estimation",
    "section": "Graphs: Histograms",
    "text": "Graphs: Histograms\n\nLet’s look at the histogram of penguin weight (body_mass_g):\n\n\n\npenguins %&gt;% \n  ggplot(aes(x=body_mass_g)) + \n  geom_histogram()"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-5",
    "href": "files/lectures/W01-L1-review-of-estimation.html#graphs-histograms-5",
    "title": "Review of Statistical Estimation",
    "section": "Graphs: Histograms",
    "text": "Graphs: Histograms\n\nLet’s look at the histogram of penguin weight (body_mass_g):\n\n\n\npenguins %&gt;% \n  ggplot(aes(x=body_mass_g)) + \n  geom_histogram() +\n  labs(x = \"Body Mass (g)\",\n       y = \"Number of Penguins\",\n       title = \"Penguin Weight Distribution\") +\n  theme_bw()"
  },
  {
    "objectID": "files/lectures/W01-L1-review-of-estimation.html#wrap-up",
    "href": "files/lectures/W01-L1-review-of-estimation.html#wrap-up",
    "title": "Review of Statistical Estimation",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we reviewed estimation.\nNext week, we will review statistical inference.\n\nConfidence intervals\nHypothesis testing\n\nGet to know you quiz - complete with RStudio.\n\n.qmd \\to Quarto\n.R \\to R script\n\nJoin the Discord server!\n\nIf you are already a Discord user, this is a friendly reminder that you can change your display name…"
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#introduction-assumptions",
    "href": "files/lectures/W04-L2-assumptions.html#introduction-assumptions",
    "title": "t-Test Assumptions",
    "section": "Introduction: Assumptions",
    "text": "Introduction: Assumptions\n\nWe have now learned one- and two-sample t-tests.\nRecall, when we have two samples, they can be independent samples or dependent samples.\n\nIndependent samples: two-sample t-test\nDependent samples: paired t-test (one-sample t-test on difference)\n\nToday we will discuss how to assess the assumptions on t-tests."
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-set-up",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-set-up",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: Set Up",
    "text": "Normality Assumption: Set Up\n\nAll t-tests assume approximate normality of the data.\n\nIn the case of one-sample t-tests, the measure of interest must somewhat follow a normal distribution.\nIn the case of two-sample t-tests, the measure of interest in each group must somewhat follow a normal distribution.\n\nNote that a paired t-test is technically a one-sample t-test, so we will examine normality of the difference."
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-set-up-1",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-set-up-1",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: Set Up",
    "text": "Normality Assumption: Set Up\n\nThere are formal tests for normality (see article here), however, we will not use them.\n\nTests for normality are not well-endorsed by statisticians.\n\nInstead, we will assess normality using a quantile-quantile (q-q) plot.\n\nThis is a scatterplot that will form a 45° line if the assumed distribution is correct.\nHere is more information about q-q plots.\n\nWe will create q-q plots for:\n\nThe measurements in the case of the one-sample t-test.\nThe measurements from each group in the case of the two-sample t-test.\nThe difference between the groups in the case of the paired t-test."
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-r-syntax",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-r-syntax",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: R Syntax",
    "text": "Normality Assumption: R Syntax\n\nWe will assess the normality assumption graphically using a q-q plot\nA package was written by a former student, classpackage.\n\nIf you are working on the server, the package is already installed.\nIf you are not working on the server, please ask me for the code needed to install."
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-independent-data---r-syntax",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-independent-data---r-syntax",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: Independent Data - R Syntax",
    "text": "Normality Assumption: Independent Data - R Syntax\n\nOnce installed, we call the package,\n\n\nlibrary(classpackage)\n\n\nWhile there are several functions in this package, we are currently interested in the independent_qq_plot() function.\n\n\ndataset_name %&gt;% independent_qq_plot(variable = \"continuous variable\",\n                                     grouping_variable = \"grouping variable\")\n\n\nThis will provide the the q-q plot for the two-sample t-test (i.e., for independent data)."
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-independent-data---example",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-independent-data---example",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: Independent Data - Example",
    "text": "Normality Assumption: Independent Data - Example\n\nRecall the penguin example for the two-sample t-test.\n\nIs the body mass different for males and females?\n\n\n\npenguins &lt;- palmerpenguins::penguins\nhead(penguins, n=3)\n\n\n  \n\n\n\n\nRequesting the q-q plot,\n\n\npenguins %&gt;% independent_qq_plot(variable = \"body_mass_g\",\n                                 grouping_variable = \"sex\")"
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-independent-data---example-1",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-independent-data---example-1",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: Independent Data - Example",
    "text": "Normality Assumption: Independent Data - Example"
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-dependent-data---r-syntax",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-dependent-data---r-syntax",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: Dependent Data - R Syntax",
    "text": "Normality Assumption: Dependent Data - R Syntax\n\nWhile there are several functions in the classpackage package, we are now interested in the dependent_qq_plot() function.\n\n\nwide_data %&gt;% dependent_qq_plot(variable = \"Display Name of Continuous Variable\",\n                                grouping_variable = \" \", # do not edit this line\n                                first_group = \"first_variable\", # first column for comparison\n                                second_group = \"second_variable\") # second column for comparison\n\n\nThis will provide the the q-q plot for the paired t-test (i.e., for dependent data)."
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-repair-estimates",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-repair-estimates",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: Repair Estimates",
    "text": "Normality Assumption: Repair Estimates\n\nRecall the repair estimate example for the dependent t-test.\n\n\ngarage &lt;- tibble(g1 = c(17.6, 20.2, 19.5, 11.3, 13.0, \n                        16.3, 15.3, 16.2, 12.2, 14.8,\n                        21.3, 22.1, 16.9, 17.6, 18.4), \n                 g2 = c(17.3, 19.1, 18.4, 11.5, 12.7, \n                        15.8, 14.9, 15.3, 12.0, 14.2, \n                        21.0, 21.0, 16.1, 16.7, 17.5))\n\n\nRequesting the q-q plot,\n\n\ngarage %&gt;% dependent_qq_plot(variable = \"estimate\",\n                             grouping_variable = \"garage\",\n                             first_group = \"g1\",\n                             second_group = \"g2\")"
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#normality-assumption-repair-estimates-1",
    "href": "files/lectures/W04-L2-assumptions.html#normality-assumption-repair-estimates-1",
    "title": "t-Test Assumptions",
    "section": "Normality Assumption: Repair Estimates",
    "text": "Normality Assumption: Repair Estimates"
  },
  {
    "objectID": "files/lectures/W04-L2-assumptions.html#wrap-up",
    "href": "files/lectures/W04-L2-assumptions.html#wrap-up",
    "title": "t-Test Assumptions",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nImportant note!!\n\nI do not expect you to agree with my assessment of q-q plots!\nWhat I do expect is that you know what to do after making your assessment.\n\nNext up:\n\nWhat happens if we do not meet the assumption for a t-test….?"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#introduction-anova-assumptions",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#introduction-anova-assumptions",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Introduction: ANOVA Assumptions",
    "text": "Introduction: ANOVA Assumptions\n\nWe previously discussed testing three or more means using ANOVA.\nWe also discussed that ANOVA is an extension of the two-sample t-test.\nRecall that the t-test has two assumptions:\n\nEqual variance between groups.\nNormal distribution.\n\nWe will extend our knowledge of checking assumptions today."
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-definition",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-definition",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Definition",
    "text": "ANOVA Assumptions: Definition\n\nWe can represent ANOVA with the following model:\n\n y_{ij} = \\mu + \\tau_i + \\varepsilon_{ij} \n\nwhere:\n\ny_{ij} is the j^{\\text{th}} observation in the i^{\\text{th}} group,\n\\mu is the overall (grand) mean,\n\\tau_i is the treatment effect for group i, and\n\\varepsilon_{ij} is the error term for the j^{\\text{th}} observation in the i^{\\text{th}} group."
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-definition-1",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-definition-1",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Definition",
    "text": "ANOVA Assumptions: Definition\n\nWe assume that the error term follows a normal distribution with mean 0 and a constant variance, \\sigma^2. i.e., \\varepsilon_{ij} \\overset{\\text{iid}}{\\sim} N(0, \\sigma^2)\nVery important note: the assumption is on the error term and NOT on the outcome!\nWe will use the residual (the difference between the observed value and the predicted value) to assess assumptions:  e_{ij} = y_{ij} - \\hat{y}_{ij}"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-graphical-assessment",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-graphical-assessment",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Graphical Assessment",
    "text": "ANOVA Assumptions: Graphical Assessment\n\nNormality: quantile-quantile plot\n\nShould have points close to the 45^\\circ line\nWe will focus on the “center” portion of the plot\n\nVariance: scatterplot of the residuals against the predicted values\n\nShould be “equal spread” between the groups\nNo “pattern”"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-graphical-assessment-1",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-graphical-assessment-1",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Graphical Assessment",
    "text": "ANOVA Assumptions: Graphical Assessment\n\nLike with t-tests, we will assess these assumptions graphically.\nWe will return to the classpackage package and use the anova_check() function.\n\n\nlibrary(classpackage) \nanova_check(m)"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-graphical-assessment-2",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-graphical-assessment-2",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Graphical Assessment",
    "text": "ANOVA Assumptions: Graphical Assessment\n\nRecall the dental example from last week,\n\n\nlibrary(tidyverse)\nstrength &lt;- c(15.4, 12.9, 17.2, 16.6, 19.3,\n              17.2, 14.3, 17.6, 21.6, 17.5,\n               5.5,  7.7, 12.2, 11.4, 16.4,\n              11.0, 12.4, 13.5,  8.9,  8.1)\nsystem &lt;- c(rep(\"Cojet\",5), rep(\"Silistor\",5), rep(\"Cimara\",5), rep(\"Ceramic\",5))\ndata &lt;- tibble(system, strength)\nm &lt;- aov(strength ~ system, data = data)\nsummary(m)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nsystem       3  200.0   66.66   7.545 0.00229 **\nResiduals   16  141.4    8.84                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-assessing-graphically",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-assessing-graphically",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Assessing Graphically",
    "text": "ANOVA Assumptions: Assessing Graphically\n\nLet’s assess the assumptions,\n\n\n\nlibrary(classpackage)\nanova_check(m)"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-test-for-variance",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-test-for-variance",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Test for Variance",
    "text": "ANOVA Assumptions: Test for Variance\n\nWe can formally check the variance assumption with the Brown-Forsythe-Levine test.\n\nThis test transforms the data and then performs ANOVA!\n\nThe test statistic is calculated as follows,  F_0 = \\frac{\\sum_{i=1}^k n_i (\\bar{z}_i - \\bar{z})^2/(k-1)}{\\sum_{i=1}^k \\sum_{j=1}^{n_j}(z_{ij}-\\bar{z}_i)^2/(n-k) },  where\n\nk is the number of groups,\nn_i is the sample size of group i,\nn = \\sum_{i=1}^k n_i, and\nz_{ij} = |y_{ij} - \\text{median}(y_i)|"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-test-for-variance-1",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-test-for-variance-1",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Test for Variance",
    "text": "ANOVA Assumptions: Test for Variance\n\nHypotheses\n\nH_0: \\ \\sigma^2_1 = ... = \\sigma^2_k\nH_1: at least one \\sigma^2_i is different\n\nTest Statistic\n\nF_0 (take from resulting ANOVA table)\n\np-Value\n\np = P[F_{\\text{df}_{\\text{Trt}}, \\text{df}_{\\text{E}}} \\ge F_0]\n\nRejection Region\n\nReject if p &lt; \\alpha."
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-test-for-variance-2",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-test-for-variance-2",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Test for Variance",
    "text": "ANOVA Assumptions: Test for Variance\n\nWe will use the leveneTest() function from the car package.\n\nNote: I do not load the car package because it overwrites a necessary function in tidyverse.\n\n\n\ncar::leveneTest(model_results)\n\n\nIn our dental example,\n\n\ncar::leveneTest(m)"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-test-for-variance-3",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#anova-assumptions-test-for-variance-3",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "ANOVA Assumptions: Test for Variance",
    "text": "ANOVA Assumptions: Test for Variance\n\nHypotheses\n\nH_0: \\ \\sigma^2_1 = \\sigma^2_2 = \\sigma^2_3 = \\sigma^2_4\nH_1: at least one \\sigma^2_i is different\n\nTest Statistic and p-Value\n\nF_0 = 0.734\np = 0.547\n\nRejection Region\n\nReject if p &lt; \\alpha; \\alpha=0.01.\n\nConclusion/Interpretation\n\nFail to reject H_0. There is not sufficient evidence to suggest that the variances are different (i.e., the variance assumption is not broken)."
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#introduction-kruskal-wallis",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#introduction-kruskal-wallis",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Introduction: Kruskal-Wallis",
    "text": "Introduction: Kruskal-Wallis\n\nWe just discussed the ANOVA assumptions.\n\n\\varepsilon_{ij} \\overset{\\text{iid}}{\\sim} N(0, \\sigma^2)\n\nWe also discussed how to assess the assumptions:\n\nGraphically using the anova_check() function.\nConfirming the variance assumption using the BFL.\n\nIf we break an assumption, we will turn to the nonparametric alternative, the Kruskal-Wallis."
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-test",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-test",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Kruskal-Wallis Test",
    "text": "Kruskal-Wallis Test\n\nIf we break ANOVA assumptions, we should implement the nonparametric version, the Kruskal-Wallis.\nThe Kruskal-Wallis test determines if k independent samples come from populations with the same distribution.\nOur new hypotheses are\n\nH_0: M_1 = ... = M_k\nH_1: at least one M_i is different"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-test-1",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-test-1",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Kruskal-Wallis Test",
    "text": "Kruskal-Wallis Test\n\nThe test statistic is as follows:\n\n \\chi^2_0 = \\frac{12}{n(n+1)} \\sum_{i=1}^k \\frac{R_i^2}{n_i} - 3(n+1), \n\nwhere\n\nR_i is the sum of the ranks for group i,\nn_i is the sample size for group i,\nn = \\sum_{i=1}^k n_i = total sample size, and\nk is the number of groups.\n\nH follows a \\chi^2 distribution with k-1 degrees of freedom."
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-test-2",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-test-2",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Kruskal-Wallis Test",
    "text": "Kruskal-Wallis Test\n\nHypotheses\n\nH_0: \\ M_1 =  ... = M_k\nH_1: at least one M_i is different\n\nTest Statistic\n\n\\chi^2_0 = \\frac{12}{n(n+1)} \\sum_{i=1}^k \\frac{R_i^2}{n_i} - 3(n+1)\n\np-Value\n\np = P[\\chi^2_{k-1} \\ge \\chi^2_0]\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-test-3",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-test-3",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Kruskal-Wallis Test",
    "text": "Kruskal-Wallis Test\n\nWe will use the kruskal.test() function to perform the Kruskal-Wallis test.\n\n\nkruskal.test(continuous_variable ~ grouping_variable, \n             data = dataset_name)\n\n\nApplying this to our dental dataset,\n\n\nkruskal.test(strength ~ system, data = data)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  strength by system\nKruskal-Wallis chi-squared = 12.515, df = 3, p-value = 0.005812"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#example",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#example",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Example",
    "text": "Example\n\nHypotheses\n\nH_0: \\ M_1 = M_2 = M_3 = M_4\nH_1: at least one M_i is different\n\nTest Statistic and p-Value\n\n\\chi_0^2 = 12.515\np = 0.006\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha=0.01.\n\nConclusion/Interpretation\n\nReject H_0. There is sufficient evidence to suggest that there is a difference in strength between the four systems."
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-posthoc-testing",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-posthoc-testing",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Kruskal-Wallis: Posthoc Testing",
    "text": "Kruskal-Wallis: Posthoc Testing\n\nWe can also perform posthoc testing in the Kruskal-Wallis setting.\nThe set up is just like Tukey’s – we can perform all pairwise comparisons and control for the Type I error rate.\nInstead of using |\\bar{y}_i - \\bar{y}_j|, we will use |\\bar{R}_i - \\bar{R}_j|, where \\bar{R}_i is the average rank of group i.\nThe comparison we are making:\n\nWe declare M_i \\ne M_j if |\\bar{R}_i - \\bar{R}_j| \\ge KW, where  KW = \\frac{q_{\\alpha}(k, \\infty)}{\\sqrt{2}} \\sqrt{\\frac{n(n+1)}{12} \\left( \\frac{1}{n_i} + \\frac{1}{n_j} \\right)}  and q_{\\alpha}(k, \\infty) is the critical value from the Studentized range distribution."
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-posthoc-testing-1",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#kruskal-wallis-posthoc-testing-1",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Kruskal-Wallis: Posthoc Testing",
    "text": "Kruskal-Wallis: Posthoc Testing\n\nWe will use the kruskalmc() function from the pgirmess package to perform the Kruskal-Wallis post-hoc test.\n\n\nkruskalmc(continuous_variable ~ grouping_variable, \n          data = dataset_name)\n\n\nIn our example,\n\n\nlibrary(pgirmess) \nkruskalmc(strength ~ system, data = data)\n\nMultiple comparison test after Kruskal-Wallis \nalpha: 0.05 \nComparisons\n                 obs.dif critical.dif stat.signif\nCeramic-Cimara       0.2     9.871455       FALSE\nCeramic-Cojet        7.9     9.871455       FALSE\nCeramic-Silistor    10.3     9.871455        TRUE\nCimara-Cojet         8.1     9.871455       FALSE\nCimara-Silistor     10.5     9.871455        TRUE\nCojet-Silistor       2.4     9.871455       FALSE\n\n\n\nWhich pairs are significantly different?"
  },
  {
    "objectID": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#wrap-up",
    "href": "files/lectures/W07-L1-ANOVA-assumptions-Kruskal-Wallis.html#wrap-up",
    "title": "ANOVA Assumptions and  Kruskal-Wallis",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we have talked about assessing ANOVA assumptions and performing the nonparametric alternative, the Kruskal-Wallis.\nPer usual, we should only look at posthoc testing when we’ve detected an overall difference with the Kruskal-Wallis.\nNext lecture: two-way ANOVA."
  },
  {
    "objectID": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#introduction",
    "href": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#introduction",
    "title": "Wilcoxon Signed Rank",
    "section": "Introduction",
    "text": "Introduction\n\nToday we have discussed that we turn to nonparametric tests when we do not meet distributional assumptions for t-tests.\nIf we do not meet the normality assumption for the paired t-test, we turn to the Wilcoxon signed rank.\nLike in the dependent t-test, we will analyze the difference between two values.\nLike in the Wilcoxon rank sum, we will be analyzing ranks."
  },
  {
    "objectID": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank",
    "href": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank",
    "title": "Wilcoxon Signed Rank",
    "section": "Wilcoxon Signed Rank",
    "text": "Wilcoxon Signed Rank\n\nBefore ranking, we will find the difference between the paired observations and eliminate any 0 differences.\n\nNote 1: elimniating 0 differences is the big difference between the other tests!\nNote 2: because we are eliminating 0 differences, this means that our sample size will update to the number of pairs with a non-0 difference.\n\nWhen ranking, we the differences are ranked based on the absolute value of the difference.\nWe also keep the sign of the difference.\n\nWe will have positive ranks and negative ranks.\n\n\n\n\n\nX\nY\nD\n|D|\nRank\n\n\n\n\n5\n8\n-3\n3\n- 1.5\n\n\n8\n5\n3\n3\n+ 1.5\n\n\n4\n4\n0\n0\n———"
  },
  {
    "objectID": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-1",
    "href": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-1",
    "title": "Wilcoxon Signed Rank",
    "section": "Wilcoxon Signed Rank",
    "text": "Wilcoxon Signed Rank\nHypotheses\n\nH_0: M_d = M_0 | H_0: M_d \\le M_0 | H_0: M_d \\ge M_0\nH_1: M_d \\ne M_0 | H_1: M_d &gt; M_0 | H_1: M_d &lt; M_0\n\nTest Statistic & p-Value\n\nT_0 = \\min(T+,|T_-|) if two-tailed, T_0 = T_+ if left-tailed, and T_0 = |T_-| if right-tailed.\np = (calculated by R :))\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha.\n\nConclusion/Interpretation\n\n[Reject or fail to reject] H_0.\nThere [is or is not] sufficient evidence to suggest [alternative hypothesis in words]."
  },
  {
    "objectID": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-2",
    "href": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-2",
    "title": "Wilcoxon Signed Rank",
    "section": "Wilcoxon Signed Rank",
    "text": "Wilcoxon Signed Rank\n\nWe will again use the wilcox.test() function to perform the test,\n\n\nwilcox.test(dataset$variable1, dataset$variable2,\n       alternative = \"alternative\",\n       mu = hypothesized_value,\n       paired = TRUE,\n       exact = FALSE)"
  },
  {
    "objectID": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-3",
    "href": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-3",
    "title": "Wilcoxon Signed Rank",
    "section": "Wilcoxon Signed Rank",
    "text": "Wilcoxon Signed Rank\n\nA stock analyst believes the median number of shares traded in Walgreens Boots Alliance (WBA) stock is greater than that in McDonald’s (MCD). Test the analyst’s belief at the \\alpha=0.05 level of significance.\n\n\nstocks &lt;- tibble(WBA = c(8.9, 6.3, 6.2, 7.2, 2.8, 3.3, 23.6, \n                         6.0, 15.6, 5.2, 6.3, 10.1, 4.0, 8.4),\n                 MCD = c(8.5, 7.6, 8.3, 10.4, 2.5, 2.6, 3.5, \n                         4.7, 9.0, 6.0, 5.6, 5.0, 4.4, 5.6))\n\n\nRecall the R syntax,\n\n\nwilcox.test(dataset$variable1, dataset$variable2,\n       alternative = \"alternative\",\n       mu = hypothesized_value,\n       paired = TRUE,\n       exact = FALSE)"
  },
  {
    "objectID": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-4",
    "href": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-4",
    "title": "Wilcoxon Signed Rank",
    "section": "Wilcoxon Signed Rank",
    "text": "Wilcoxon Signed Rank\n\nFrom the problem statement: A stock analyst believes the median number of shares traded in Walgreens Boots Alliance (WBA) stock is greater than that in McDonald’s (MCD).\n\n\nwilcox.test(stocks$WBA, stocks$MCD,\n       alternative = \"greater\",\n       paired = TRUE, \n       exact = FALSE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  stocks$WBA and stocks$MCD\nV = 69, p-value = 0.1575\nalternative hypothesis: true location shift is greater than 0"
  },
  {
    "objectID": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-5",
    "href": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wilcoxon-signed-rank-5",
    "title": "Wilcoxon Signed Rank",
    "section": "Wilcoxon Signed Rank",
    "text": "Wilcoxon Signed Rank\n\nHypotheses\n\nH_0: \\ M_{\\text{WBA}} \\le M_{\\text{MCD}} OR M_d \\le 0, where d = \\text{WBA} - \\text{MCD}\nH_1: \\ M_{\\text{WBA}} &gt; M_{\\text{MCD}} OR M_d &gt; 0\n\nTest Statistic and p-Value\n\nV_0 = 69\np = 0.158\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha = 0.05.\n\nConclusion/Interpretation\n\nFail to reject H_0.\nThere is not sufficient evidence to suggest that the median stock shares traded is greater for WBA than for MCD."
  },
  {
    "objectID": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wrap-up",
    "href": "files/lectures/W04-L4-Wilcoxon-signed-rank.html#wrap-up",
    "title": "Wilcoxon Signed Rank",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we reviewed the Wilcoxon signed rank test.\n\nNonparametric alternative to the paired t-test.\n\nThis completes Module 1.\nNext: comparing three or more groups."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#introduction",
    "href": "files/lectures/W02-L1-two-sample-t.html#introduction",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Introduction",
    "text": "Introduction\n\nIn the last lecture, we focused on describing data.\n\nContinuous data: mean with standard deviation, median with interquartile range\nCategorical data: count with percentage\n\nToday, we will focus on drawing conclusions about two population means using data.\n\nConfidence intervals\nHypothesis testing"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-intervals",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-intervals",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\n\n\n\n\nPoint Estimate\n\n\nThe single value of a statistic that estimates the value of a parameter.\n\n\n\n\nExamples of point estimates:\nIt is necessary to know how good our estimation is, or to quantify our uncertainty.\n\n\n\n\n\n\n\nConfidence Interval\n\n\nA range of plausible values for the parameter based on values observed in the sample.\n\n\\text{estimate} \\pm \\text{margin of error}\n\n\n\n\n\n\n\n\n\n\nLevel of Confidence\n\n\nThe probability that the interval will capture the true parameter value in repeated samples. i.e., the success rate for the method."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-intervals-1",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-intervals-1",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-intervals-2",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-intervals-2",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nBecause CIs are a range of values, we will use interval notation,\n\n\n(lower bound, upper bound)\n\n\nwhere\n\nlower bound = point estimate – margin of error\nupper bound = point estimate + margin of error\n\nMake sure to state your confidence intervals in numeric order.\n\ni.e., the lower bound must be the smaller number and the upper bound must be the larger number."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}",
    "text": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}\n\n\n\n\n\n\n(1-\\alpha)100\\% confidence interval for \\mu_1-\\mu_2\n\n\n\n(\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2} \\sqrt{\\frac{s_1^2 }{n_1} + \\frac{s_2^2}{n_2}}\n where t_{\\alpha/2} has \\text{min}(n_1-1, n_2-1) degrees of freedom.\n\n\n\n\nTo construct this interval, we require either:\n\nthe two populations to be normally distributed or\nthe sample sizes are sufficiently large (n_1 \\ge 30 and n_2 \\ge 30)\n\nR syntax:\n\n\nt.test(continuous_variable ~ grouping_variable,\n       data = dataset_name,\n       conf.level = confidence_level)"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-1",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-1",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}",
    "text": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}\n\nRecall the Palmer penguin data,\n\n\npenguins &lt;- palmerpenguins::penguins"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-2",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-2",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}",
    "text": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}\n\nLet’s find the 95% confidence interval for the difference in average weight (body_mass_g) between male and female (sex) penguins.\nRemember the R syntax:\n\n\nt.test(continuous_variable ~ grouping_variable,\n       data = dataset_name,\n       conf.level = confidence_level) \n\n\nWhat is the continuous variable?\nWhat is the grouping variable?\nWhat is the dataset name?\nWhat is the confidence level?"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-3",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-3",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}",
    "text": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}\n\nLet’s find the 95% confidence interval for the difference in weight (body_mass_g) between male and female penguins.\n\n\nt.test(body_mass_g ~ sex,\n       data = penguins,\n       conf.level = 0.95) \n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by sex\nt = -8.5545, df = 323.9, p-value = 4.794e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -840.5783 -526.2453\nsample estimates:\nmean in group female   mean in group male \n            3862.273             4545.685 \n\n\n\nThus, the 95% confidence interval for \\mu_F - \\mu_M is (-840.6, -526.2)."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-4",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-4",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}",
    "text": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}\n\nWhat about the 99% confidence interval for the difference in weight (body_mass_g) between male and female penguins?\n\n\nt.test(body_mass_g ~ sex,\n       data = penguins,\n       conf.level = 0.99) \n\n\nWhat do you expect? Recall that the 95% CI was (-840.6, -526.2)."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-5",
    "href": "files/lectures/W02-L1-two-sample-t.html#confidence-interval-for-mathbfboldsymbol-mu_1-boldsymbolmu_2-5",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}",
    "text": "Confidence Interval for \\mathbf{\\boldsymbol \\mu_1-\\boldsymbol\\mu_2}\n\nWhat about the 99% confidence interval for the difference in weight (body_mass_g) between male and female penguins?\n\n\nt.test(body_mass_g ~ sex,\n       data = penguins,\n       conf.level = 0.99) \n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by sex\nt = -8.5545, df = 323.9, p-value = 4.794e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n99 percent confidence interval:\n -890.4112 -476.4124\nsample estimates:\nmean in group female   mean in group male \n            3862.273             4545.685 \n\n\n\nThus, the 99% confidence interval for \\mu_F - \\mu_M is (-890.4, -476.4).\n\nRecall that the 95% CI was (-840.6, -526.2)."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nA friend of yours wants to play a simple coin-flipping game.\n\nIf the coin comes up heads, you win; if it comes up tails, your friend wins.\nSuppose the outcome of five plays of the game is T, T, T, T, T.\nIs your friend cheating?"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-1",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-1",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nA friend of yours wants to play a simple coin-flipping game.\n\nIf the coin comes up heads, you win; if it comes up tails, your friend wins.\nSuppose the outcome of five plays of the game is T, T, T, T, T.\nIs your friend cheating?\n\nWe know the probability of flipping a tail is 0.5.\nWe can compute the probability of flipping five tails in a row.\n\n\n\n\n\\begin{align*}\n     P[\\text{T, T, T, T, T}] &=  0.5 \\times 0.5 \\times 0.5 \\times 0.5 \\times 0.5 \\\\\n     &= 0.03125\n\\end{align*}\n\n\nIs this probability low enough to believe your friend is cheating?"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-2",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-2",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\n\n\n\n\n\nHypothesis Testing\n\n\nA procedure, based on sample evidence and probability, used to test statements regarding a characteristic of one or more populations.\n\n\n\n\nSteps in hypothesis testing\n\nMake a statement regarding the nature of the population.\nCollect evidence (sample data) to test the statement.\nAnalyze the data to assess the plausibility of the statement.\n\nNote: if we have population parameters available, we do not need to perform a hypothesis test."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-hypotheses",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-hypotheses",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: Hypotheses",
    "text": "Hypothesis Testing: Hypotheses\n\n\n\n\n\n\nHypothesis\n\n\nA statement regarding a characteristic of one or more populations.\n\n\n\n\nIn hypothesis testing, we have two hypotheses: the null and the alternative.\n\n\n\n\n\n\n\nNull hypothesis, H_0\n\n\nA statement to be tested.\n\nThis is a statement of no change, no effect, or no difference.\nIt is assumed true until evidence indicates otherwise.\n\n\n\n\n\n\n\n\n\n\nAlternative hypothesis, H_1\n\n\nA statement that we are trying to find evidence to support."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-hypotheses-1",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-hypotheses-1",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: Hypotheses",
    "text": "Hypothesis Testing: Hypotheses\n\nOne sample tests:\n\nTwo-tailed test\n\nH_0: parameter = some value\nH_1: parameter \\ne some value\n\nLeft-tailed test\n\nH_0: parameter \\ge some value\nH_1: parameter &lt; some value\n\nRight-tailed test\n\nH_0: parameter \\le some value\nH_1: parameter &gt; some value"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-hypotheses-2",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-hypotheses-2",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: Hypotheses",
    "text": "Hypothesis Testing: Hypotheses\n\nTwo sample tests\n\nTwo-tailed test\n\nH_0: parameter1 – parameter2 = 0\nH_1: parameter1 – parameter2 \\ne 0\n\nLeft-tailed test\n\nH_0: parameter1 – parameter2 \\ge 0\nH_1: parameter1 – parameter2 &lt; 0\n\nRight-tailed test\n\nH_0: parameter1 – parameter2 \\le 0\nH_1: parameter1 – parameter2 &gt; 0"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-errors",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-errors",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: Errors",
    "text": "Hypothesis Testing: Errors\n\nWe use data to draw conclusions about hypotheses.\n\nWe will either reject or fail to reject the null (H_0).\n\nIf we draw the wrong conclusion, we make an error.\nThese can be classified as Type I (\\alpha) or Type II (\\beta) errors.\n\n\\alpha and \\beta are probabilities (i.e., are between 0 and 1)."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-errors-1",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-errors-1",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: Errors",
    "text": "Hypothesis Testing: Errors\n\nAs stated earlier, Type I (\\alpha) and Type II (\\beta) errors are probabilities.\n\n\\alpha = \\text{P}[\\text{reject } H_0 \\text{ when } H_0 \\text{ is true}]\n\\beta = \\text{P}[\\text{fail to reject } H_0 \\text{ when } H_1 \\text{ is true}]\n\nWe also call \\alpha the level of significance.\nWe should choose \\alpha based on the level of error we are willing to withstand in the experiment.\n\nThe \\alpha that is commonly used is \\alpha=0.05.\nSometimes, smaller \\alpha is used. e.g., clinical trial \\to \\alpha=0.01.\n\nFor a fixed sample size (n), \\alpha and \\beta are inversely related."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-test-statistics",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-test-statistics",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: Test Statistics",
    "text": "Hypothesis Testing: Test Statistics\n\nAfter stating our hypotheses, we will construct a test statistic.\nThe choice of test statistic depends on:\n\nThe hypotheses being tested.\nAssumptions made about the data.\n\nThe value of the test statistic depends on the sample data.\n\nIf we were to draw a different sample, we would find a different value for the test statistic.\n\nWe will use the test statistic on our way to drawing conclusions about the hypotheses."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-p-values",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-p-values",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: p-Values",
    "text": "Hypothesis Testing: p-Values\n\n\n\n\n\n\np-value\n\n\nThe probability of observing what we’ve observed or something more extreme, assuming the null hypothesis is true.\n\n\n\n\nAfter constructing test statistics, we will find the corresponding p-value.\nFinding a p-value depends on the distribution being used.\nWe will compare the p-value to \\alpha in order to draw conclusions.\n\nReject H_0 if p &lt; \\alpha."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-conclusions-and-interpretations",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-conclusions-and-interpretations",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: Conclusions and Interpretations",
    "text": "Hypothesis Testing: Conclusions and Interpretations\n\nOnce we’ve found the p-value, we can draw a conclusion.\n\nIf p &lt; \\alpha, we reject H_0.\n\nThere is sufficient evidence to suggest that H_1 is true.\n\nIf p \\ge \\alpha, we fail to reject H_0.\n\nThere is not sufficient evidence to suggest that H_1 is true.\n\n\nTake aways:\n\nWe never “accept” the null.\nWe always interpret in terms of H_1."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test",
    "href": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Two-Sample t-Test",
    "text": "Two-Sample t-Test\n\n\n\n\n\n\nHypothesis Test for Two Independent Means\n\n\nHypotheses\n\nH_0: \\mu_1-\\mu_2 = \\mu_0 | H_0: \\mu_1-\\mu_2 \\le \\mu_0 | H_0: \\mu_1-\\mu_2 \\ge \\mu_0\nH_1: \\mu_1-\\mu_2 \\ne \\mu_0 | H_0: \\mu_1 - \\mu_2 &gt; \\mu_0 | H_1: \\mu_1 - \\mu_2 &lt; \\mu_0\n\nTest Statistic \nt_0 = \\frac{(\\bar{x}_1-\\bar{x}_2)-\\mu_0}{\\sqrt{\\frac{s_1^2}{n}+\\frac{s_2^2}{n}}}\n\np-Value\n\np = 2 P[t \\ge |t_0|] | p = P[t \\ge |t_0|] | p = P[t \\le |t_0|]\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha.\n\nConclusion/Interpretation\n\n[Reject or fail to reject] H_0.\nThere [is or is not] sufficient evidence to suggest [alternative hypothesis in words]."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test-1",
    "href": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test-1",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Two-Sample t-Test",
    "text": "Two-Sample t-Test\n\nR syntax:\n\n\nt.test(continuous_variable ~ grouping_variable,\n       data = dataset_name,\n       mu = hypothesized_difference,\n       alternative = alternative)\n\n\nImportant!!\n\nWe are estimating \\mu_1 - \\mu_2, but R is going to subtract in alphabetical or numeric order of the grouping variable.\n\ne.g., if we have “Male” and “Female”, it will estimate \\mu_{\\text{Female}} - \\mu_{\\text{Male}}.\ne.g., if we have “110” and “5”, it will estimate \\mu_{5} - \\mu_{110}.\nIn the case of two-tailed tets, this does not matter… but beware when doing a one-tailed test!"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test-example",
    "href": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test-example",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Two-Sample t-Test: Example",
    "text": "Two-Sample t-Test: Example\n\nConsider the penguin data. Is there a significant difference in weight (body_mass_g) between male and female penguins? Test at the \\alpha=0.05 level.\nRemember the R syntax:\n\n\nt.test(continuous_variable ~ grouping_variable,\n       data = dataset_name,\n       mu = hypothesized_difference,\n       alternative = alternative)\n\n\nWhat is the continuous variable?\nWhat is the grouping variable?\nWhat is the dataset name?\nWhat is the hypothesized difference?\nWhat is the alternative?"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test-example-1",
    "href": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test-example-1",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Two-Sample t-Test: Example",
    "text": "Two-Sample t-Test: Example\n\nConsider the penguin data. Is there a significant difference in weight (body_mass_g) between male and female penguins? Test at the \\alpha=0.05 level.\nRemember the R syntax:\n\n\nt.test(body_mass_g ~ sex,\n       data = penguins,\n       mu = 0,\n       alternative = \"two\")\n\n\n    Welch Two Sample t-test\n\ndata:  body_mass_g by sex\nt = -8.5545, df = 323.9, p-value = 4.794e-16\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -840.5783 -526.2453\nsample estimates:\nmean in group female   mean in group male \n            3862.273             4545.685 \n\n\n\nIs this a significant difference?"
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test-example-2",
    "href": "files/lectures/W02-L1-two-sample-t.html#two-sample-t-test-example-2",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Two-Sample t-Test: Example",
    "text": "Two-Sample t-Test: Example\nHypotheses\n\nH_0: \\mu_1-\\mu_2 = 0\nH_1: \\mu_1-\\mu_2 \\ne 0\n\nTest Statistic and p-Value\n\nt_0 = -8.55\np &lt; 0.001\n\nRejection Region\n\nReject H_0 if p &lt; \\alpha; \\alpha=0.05.\n\nConclusion/Interpretation\n\nReject H_0.\nThere is sufficient evidence to suggest that male and female penguins have different weights."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-practical-vs.-statistical-significance",
    "href": "files/lectures/W02-L1-two-sample-t.html#hypothesis-testing-practical-vs.-statistical-significance",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Hypothesis Testing: Practical vs. Statistical Significance",
    "text": "Hypothesis Testing: Practical vs. Statistical Significance\n\nHypothesis testing depends on sample size.\nAs the sample size increases, our p-values decrease necessarily.\nAs p-values decrease, we are more likely to reject the null hypothesis.\nWe must ask ourselves if the value we are testing against makes practical sense.\n\nA new weight loss medication where the average amount of weight loss was 1 lb over 6 months.\nA new weight loss medication where the average amount of weight lost was 15 lb over 6 months.\nA new teaching method that raised final exam scores by 2 points.\nA new teaching method that raised final exam scores by 15 points."
  },
  {
    "objectID": "files/lectures/W02-L1-two-sample-t.html#wrap-up",
    "href": "files/lectures/W02-L1-two-sample-t.html#wrap-up",
    "title": "Review: Inferential Statistics & Two-Sample t-Tests",
    "section": "Wrap Up",
    "text": "Wrap Up\n\nToday we reviewed statistical inference.\n\nConfidence intervals\nHypothesis testing\n\nGet to know you quiz - complete with RStudio - due today.\nNext meeting: how to conceptualize research questions; dependent t-test."
  }
]