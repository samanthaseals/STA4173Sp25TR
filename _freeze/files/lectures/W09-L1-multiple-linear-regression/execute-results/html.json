{
  "hash": "b4db7981aa5930619c52dc0c2da108e5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Multiple Regression**\"\nsubtitle: \"**STA4173: Biostatistics** <br> Spring 2025\"\nformat: \n  revealjs: \n    code-overflow: wrap\n    df-print: paged\n    embed-resources: true\n    slide-number: true\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\neditor: source\n---\n\n\n\n## Introduction\n\n- We have learned simple linear regression, $$y = \\beta_0 + \\beta_1 x + \\varepsilon$$\n\n- Today, we will expand to multiple regression, which allows us to include multiple predictors, $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\varepsilon$$\n\n    - Simple linear regression is just a special case of multiple regression, where $k=1$.\n    \n- The good news is that all things we learned for simple linear regression hold true for multiple regression! ðŸ˜Ž\n\n## R Syntax\n\n- We again use the `lm()` function to define the model and `summary()` to see the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(outcome ~ predictor_1 + predcitor_2 + ... + predictor_k, \n        data = dataset_name)\nsummary(m)\n```\n:::\n\n\n\n- Additionally, we will find the confidence intervals for the $\\beta_i$ using `confint()`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m) # for 95% CI\nconfint(m, level = conf_level) # for other levels\n```\n:::\n\n\n\n##  Interpretations\n\n- We interpret coefficients much the same as in simple linear regression.\n\n    - Intercept: when [*all* predictors = 0], the average [outcome] is [$\\hat{\\beta}_0$].\n\n    - Slope: For a 1 [units of predictor $i$] increase in [predictor $i$], we expect [outcome] to [increase or decrease] by [$|\\hat{\\beta}_i|$] [units of outcome], after controlling for [all other predictors in the model].\n    \n        - If $\\hat{\\beta}_i > 0$, there is an increase.\n        - If $\\hat{\\beta}_i < 0$, there is a decrease.\n\n##  Example\n\n- A family doctor wishes to further examine the variables that affect their female patients' total cholesterol. \n\n- They randomly select 14 female patients, measure their total cholesterol, and asks the patients to determine their average daily consumption of saturated fat. \n\n- The data is as follows,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndata <- tibble(age = c(25, 25, 28, 32, 32, 32, 38, 42, 48, 51, 51, 58, 62, 65), \n               chol = c(180, 195, 186, 180, 210, 197, 239, 183, 204, 221, 243, 208, 228, 269), \n               fat = c(19, 28, 19, 16, 24, 20, 31, 20, 26, 24, 32, 21, 21, 30))\nhead(data, n = 2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"age\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"chol\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"fat\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"25\",\"2\":\"180\",\"3\":\"19\"},{\"1\":\"25\",\"2\":\"195\",\"3\":\"28\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n##  Example\n\n- Model total cholesterol ($y$) as a function of age ($x_1$) and daily consumption of saturated fat ($x_2$).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(chol ~ age + fat, data = data)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = chol ~ age + fat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.874  -8.192   3.479   8.151  14.907 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  90.8415    15.9887   5.682 0.000142 ***\nage           1.0142     0.2427   4.179 0.001540 ** \nfat           3.2443     0.6632   4.892 0.000478 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.42 on 11 degrees of freedom\nMultiple R-squared:  0.8473,\tAdjusted R-squared:  0.8196 \nF-statistic: 30.53 on 2 and 11 DF,  p-value: 3.239e-05\n```\n\n\n:::\n:::\n\n\n\n- The resulting model is $$\\hat{\\text{cholesterol}}_i = 90.842 + 1.014 \\text{ age}_i + 3.244 \\text{ fat}_i$$\n\n##  Example\n\n- Let's now interpret the slopes from the cholesterol example. $$\\hat{\\text{cholesterol}}_i = 90.842 + 1.014 \\text{ age}_i + 3.244 \\text{ fat}_i$$\n\n    - For a 1 year increase in age, the cholesterol level is expected to increase by 1.014 mg/dL, after adjusting for the average daily consumption of saturated fat.\n    \n    - For a 1 gram increase in average daily consumption of saturated fat, the total cholesterol is expected to increase by 3.244 mg/dL, after adjusting for age.\n\n##  Significance of $\\beta_i$\n\n- **Hypotheses**\n\n    - $H_0: \\ \\beta_i = 0$ \n    - $H_1: \\ \\beta_i \\ne 0$\n\n- **Test Statistic**\n\n    - $t_0 = \\frac{\\hat{\\beta}_i}{\\text{SE of }\\hat{\\beta}_i}$\n\n- ***p*-Value**\n\n    - $p = 2 \\times P[t_{n-k-1} \\ge |t_0|]$\n\n- **Rejection Region**\n\n    - Reject $H_0$ if $p<\\alpha$.\n\n##  Example\n\n- Which, if any, are significant predictors of cholesterol?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = chol ~ age + fat, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.874  -8.192   3.479   8.151  14.907 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  90.8415    15.9887   5.682 0.000142 ***\nage           1.0142     0.2427   4.179 0.001540 ** \nfat           3.2443     0.6632   4.892 0.000478 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.42 on 11 degrees of freedom\nMultiple R-squared:  0.8473,\tAdjusted R-squared:  0.8196 \nF-statistic: 30.53 on 2 and 11 DF,  p-value: 3.239e-05\n```\n\n\n:::\n:::\n\n\n\n##  Example {.smaller}\n\n- **Hypotheses**\n\n    - $H_0: \\ \\beta_{\\text{age}} = 0$ \n    - $H_1: \\ \\beta_{\\text{age}} \\ne 0$\n\n- **Test Statistic and *p*-Value**\n\n    - $t_0 = 4.179$\n\n    - $p = 0.002$\n\n- **Rejection Region**\n\n    - Reject $H_0$ if $p<\\alpha$; $\\alpha=0.05$\n    \n- **Conclusion/Interpretation**\n\n    - Reject $H_0$.\n    \n    - There is sufficient evidence to suggest that there is a relationship between age and cholesterol.\n\n##  Example {.smaller}\n\n- **Hypotheses**\n\n    - $H_0: \\ \\beta_{\\text{fat}} = 0$ \n    - $H_1: \\ \\beta_{\\text{fat}} \\ne 0$\n\n- **Test Statistic and *p*-Value**\n\n    - $t_0 = 4.892$\n\n    - $p < 0.001$\n\n- **Rejection Region**\n\n    - Reject $H_0$ if $p<\\alpha$; $\\alpha=0.05$\n    \n- **Conclusion/Interpretation**\n\n    - Reject $H_0$.\n    \n    - There is sufficient evidence to suggest that there is a relationship between fat and cholesterol.\n\n##  Example\n\n- Let's find the 95% confidence intervals for the regression coefficients.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5 %     97.5 %\n(Intercept) 55.6507255 126.032306\nage          0.4800054   1.548405\nfat          1.7845942   4.703937\n```\n\n\n:::\n:::\n\n\n\n- The 95% CI for $\\beta_{\\text{age}}$ is (0.48, 1.55).\n\n- The 95% CI for $\\beta_{\\text{fat}}$ is (1.78, 4.70).\n\n##  Reporting Results\n\n- How do I report regression models in the real world? \n\n    - I always give a table of $\\hat{\\beta}_i$, (95% CI), and $p$-values.\n    \n- In our example,\n\n| Predictor | Estimate (95% CI) | *p*-Value |\n|-|:-:|-:|\n| Age | 1.01 (0.48, 1.55) | 0.002 |\n| Fat | 3.24 (1.78, 4.70) | < 0.001 |\n\n##  Significant Regression Line\n\n- We can use an $F$-test to test for a significant regression line. \n\n    - A significant regression line means that there is a non-zero slope among all slopes. \n\n    - This makes use of an ANOVA table, however, we will not concern ourselves with the computation.\n    \n- The results can be found at the bottom of the `summary()` output.\n\n##  Significant Regression Line\n\n- **Hypotheses**\n\n    - $H_0: \\ \\beta_1 = ... = \\beta_k = 0$ \n    - $H_1:$ at least one is different\n\n- **Test Statistic**\n\n    - $F_0$ (pulled from the `summary()` output)\n\n- ***p*-Value**\n\n    - $p = \\text{P}[F_{k, n-k-1} \\ge F_0]$\n\n- **Rejection Region**\n\n    - Reject $H_0$ if $p < \\alpha$.\n\n##  Example\n\n- Is this a significant regression line?\n\n    - Is at least one of the $x_i$ a significant predictor?\n    \n    - Is at least one slope non-zero?\n\n<center><img src = \"images/W09-L1-a.png\" width=900></center>\n\n##  Example\n\n- **Hypotheses**\n\n    - $H_0: \\ \\beta_1 = \\beta_2 = 0$ \n    - $H_1:$ at least one is different\n\n- **Test Statistic and *p*-Value**\n\n    - $F_0 = 30.53$ \n\n    - $p < 0.001$\n\n- **Rejection Region**\n\n    - Reject $H_0$ if $p < \\alpha$; $\\alpha=0.05$.\n    \n- **Conclusion/Interpretation**\n\n    - Reject $H_0$.\n    - There is sufficient evidence to suggest that at least one slope is non-zero. \n\n##  Line Fit\n\n- We can assess how well the regression model fits the data using $R^2$. $$ R^2 = \\frac{\\text{SS}_{\\text{Reg}}}{\\text{SS}_{\\text{Tot}}} $$\n\n- Thus, $R^2$ is the proportion of variation explained by the model (i.e., predictor set).\n\n- $R^2 \\in [0, 1]$\n\n    - $R^2 \\to 0$ indicates that the model fits \"poorly.\"\n    \n    - $R^2 \\to 1$ indicates that the model fits \"well.\"\n\n    - $R^2 = 1$ indicates that all points fall on the response surface.\n\n##  Line Fit\n\n- Recall that the error term in ANOVA is the \"catch all\" ...\n\n    - The SS<sub>Tot</sub> is constant for the outcome of interest.\n    \n    - As we add predictors to the model, we are necessarily increasing SS<sub>Reg</sub>\n    \n        - The variance is moving from SS<sub>E</sub> to SS<sub>Reg</sub>\n        \n- We do not want to arbitrarily increase $R^2$, so we will use an adjusted version:$$ R^2_{\\text{adj}} = 1 - \\frac{\\text{MS}_{\\text{E}}}{\\text{SS}_{\\text{Tot}}/\\text{df}_{\\text{Tot}}}$$\n\n##  Example\n\n- The $R^2$ and $R^2_{\\text{adj}}$ both come out of the `summary()` function.\n\n<center><img src = \"images/W09-L1-a.png\" width=900></center>\n\n- $R^2$ is 0.847.\n\n- $R^2_{\\text{adj}}$ is 0.820 -- 82.0% of the variability in cholesterol is explained by the model with age and fat.\n\n\n##  Outliers\n\n- Definition: data values that are much larger or smaller than the rest of the values in the dataset.\n\n- We will look at the standardized residuals, $$ e_{i_{\\text{standardized}}} = \\frac{e_i}{\\sqrt{\\text{MSE}(1-h_i)}}, $$ where\n\n    - $e_i = y_i - \\hat{y}_i$ is the residual of the $i$^th^ observation\n    - $h_i$ is the leverage of the $i$^th^ observation\n    \n- If $|e_{i_{\\text{standardized}}}| > 2.5 \\ \\to \\ $ outlier.\n\n- If $|e_{i_{\\text{standardized}}}| > 3 \\ \\to \\ $ extreme outlier.\n\n##  Outliers\n\n- We will use the `rstandard()` function to find the residuals, then we will count the outliers,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_name %>% count(abs(rstandard(m))>2.5)\n```\n:::\n\n\n\n- In our example data,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% count(abs(rstandard(m))>2.5)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"abs(rstandard(m)) > 2.5\"],\"name\":[1],\"type\":[\"lgl\"],\"align\":[\"right\"]},{\"label\":[\"n\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"FALSE\",\"2\":\"14\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n- There are no outliers.\n\n    - (This is Happy Textbook Land!)\n\n##  Multicollinearity\n\n- Collinearity/multicollinearity: a correlation between two or more predictor variables affects the estimation procedure.\n\n- We will use the variance inflation factor (VIF) to check for multicollinearity. $$ \\text{VIF}_j = \\frac{1}{1-R^2_j}, $$\n\n- where\n\n    - $j$ = the predictor of interest and $j \\in \\{1, 2, ..., k \\}$,\n    - $R^2_j$ results from regressing $x_j$ on the remaining $(k-1)$ predictors.\n    \n- We say that multicollinearity is present if VIF > 10.\n\n##  Multicollinearity\n\n- How do we deal with multicollinearity?\n\n    - Easy answer: remove at least one predictor from the collinear set, then reassess VIF.\n    \n    - More complicated: how do we know which predictor should be the one removed?\n    \n        - (We will likely need to consult with the research team.)\n    \n##  Multicollinearity\n\n- We will use the [`vif()`](https://www.rdocumentation.org/packages/car/versions/3.1-0/topics/vif) function from the `car` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(m)\n```\n:::\n\n\n\n- There will be a value for each predictor in the model.\n- In our cholesterol model,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncar::vif(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     age      fat \n1.117404 1.117404 \n```\n\n\n:::\n:::\n\n\n\n- No multicollinearity is present.\n\n## Conclusions\n\n- This is just scratching the surface for multiple regression.\n\n- Other statistics courses go deeper into regression topics.\n\n    - Categorical predictors.\n    \n    - Interaction terms.\n    \n    - Other regression diagnostics.\n    \n    - How to handle non-continuous outcomes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "W09-L1-multiple-linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}