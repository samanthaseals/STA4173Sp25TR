{
  "hash": "a4e9815811ac2f84a5811e343aef177b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"**Correlation and <br>Simple Linear Regression**\"\nsubtitle: \"**STA4173: Biostatistics** <br> Spring 2025\"\nformat: \n  revealjs: \n    code-overflow: wrap\n    df-print: paged\n    embed-resources: true\n    slide-number: true\n    width: 1600\n    height: 900\n    html-math-method: katex\n    theme:\n      - default\nexecute:\n  echo: true\n  warning: false\n  message: false\n  error: true\neditor: source\n---\n\n\n\n## Introduction: Correlation\n\n- Before today, we discussed methods for comparing continuous outcomes across two or more groups.\n\n- We now will begin exploring the relationships between two *continuous* variables. \n\n- We will first focus on data visualization and correlation.\n\n- The we will quantify the relationship using regression analysis.\n\n## Scatterplot\n\n- **Scatterplot or scatter diagram**:\n\n    - A graph that shows the relationship between two quantitative variables measured on the same subject.\n    \n- Each individual in the dataset is represented by a point on the scatterplot. \n\n- The explanatory variable is on the $x$-axis and the response variable is on the $y$-axis.\n\n- It is super important for us to plot the data! \n\n    - Plotting the data is a first step in identifying issues or potential relationships.\n    \n## Scatterplots\n\n- **Positive relationship**: As $x$ increases, $y$ increases.\n\n- **Negative relationship**: As $x$ increases, $y$ decreases.\n\n<center>\n<img src=\"images/L14a.png\" width = 950> <br>\n<img src=\"images/L14b.png\" width = 575>\n</center>\n    \n## Example\n\n- A golf pro wants to investigate the relation between the club-head speed of a golf club (measured in miles per hour) and the distance (in yards) that the ball will travel. \n\n- The pro uses a single model of club and ball, one golfer, and a clear, 70-degree day with no wind. \n\n- The pro records the club-head speed, measures the distance the ball travels, and collected the data below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ngolf <- tibble(speed = c(100, 102, 103, 101, 105, 100, 99, 105),\n               distance = c(257, 264, 274, 266, 277, 263, 258, 275))\n  head(golf, n=3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"speed\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"distance\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"100\",\"2\":\"257\"},{\"1\":\"102\",\"2\":\"264\"},{\"1\":\"103\",\"2\":\"274\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## Example\n\n- Like we have done before, we will graph using the `ggplot2` package.\n\n<center>\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngolf %>% ggplot(aes(x = speed, y = distance)) + \n  geom_point(size = 5) + # plot points; make dots bigger\n  labs(x = \"Club Head Speed (mph)\",\n       y = \"Distance (yards)\") + # define labels \n  theme_bw() # change background color\n```\n\n::: {.cell-output-display}\n![](W08-L1-simple-linear-regression_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n</center>\n\n## Correlation\n\n- Creating the scatterplot allows us to visualize a potential relationship.\n\n    - e.g., we know from the scatterplot for the golf data, as speed increases, distance increases.\n    \n- Now, let's talk about how to quantify that relationship.\n\n    - Initial quantification: correlation.\n    \n    - Further quantification: regression.\n\n## Correlation\n\n- **Correlation**: A measure of the strength and direction of the linear relationship between two quantitative variables. \n\n    - $\\rho$ represents the population correlation coefficient.\n    \n    - $r$ represents the sample correlation coefficient. \n\n- Correlation is bounded to $[-1, 1]$.\n\n    - $r=-1$ represents perfect negative correlation.\n    \n    - $r=1$ represents perfect positive correlation.\n    \n    - $r=0$ represents no correlation.\n\n## Pearsons's Correlation\n\n- Pearson's correlation coefficient: \n\n$$r = \\frac{\\sum_{i=1}^n \\left( \\frac{x_i - \\bar{x}}{s_x} \\right)\\left( \\frac{y_i - \\bar{y}}{s_y} \\right)}{n-1}$$ \n\n- $x_i$ is the *i*^th^ observation of $x$; $y_i$ is the *i*^th^ observation of $y$\n- $\\bar{x}$ is the sample mean of $x$; $\\bar{y}$ is the sample mean of $y$\n- $s_x$ is the sample standard deviation of $x$; $s_y$ is the sample standard deviation of $y$\n- $n$ is the sample size (number of paired observations)\n\n## Pearsons's Correlation\n\n- In R, we will use the [`cor()` function](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor) to find the correlation.\n\n    - We can plug in the two specific variables we are interested in.\n    \n    - We can also plug in the dataset we are working with and all pairwise correlations will be reporeted. \n    \n        - (This will be useful later!)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(dataset_name$variable_1, dataset_name$variable_2) # request correlation between two values \ncor(dataset_name) # request correlation from a tibble (dataset)\n```\n:::\n\n\n\n## Example\n\n- Let's find the correlation of the golf data.\n\n- First, let's plug in the specific variables we are interested in.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(golf$distance, golf$speed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9386958\n```\n\n\n:::\n:::\n\n\n\n- Looking at the correlation coefficient for distance and speed, $r = 0.94$.\n\n    - This is a strong positive correlation.\n\n## Example\n\n- Now, let's plug the whole dataset in.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(golf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             speed  distance\nspeed    1.0000000 0.9386958\ndistance 0.9386958 1.0000000\n```\n\n\n:::\n:::\n\n\n\n- Notes:\n\n    - The correlation between a variable and itself is 1.\n    \n        - We can see this above: $r_{\\text{speed, speed}} = 1$ and $r_{\\text{distance, distance}} = 1$.\n    \n    - The correlation between $x$ and $y$ is the same as the correlation between $y$ and $x$.\n    \n        - We can see this above: $r_{\\text{speed, distance}} = r_{\\text{distance, speed}}$ \n        \n## Spearman's Correlation\n\n- An assumption for Pearson's correlation is that both $x$ and $y$ are normally distributed. \n- What do we do when we do not meet the normality assumption?\n\n- **Spearman's Correlation**: A measure of the strength and direction of the monotone relationship between two variables. \n\n    - Spearman's correlation only assumes that the data is ordinal. \n\n    - It does not work for nominal data!\n\n- Spearman's correlation is interpreted the same as Pearson's correlation.\n\n- To find Spearman's correlation, the following algorithm is followed:\n\n    1. Rank the $x$ and $y$ values. \n    \n        - ($x$, $y$) $\\to$ ($R_x$, $R_y$)\n    \n    2. Find Pearson's correlation for the ranked data. \n    \n## Spearman's Correlation\n\n- We will again use the `cor()` function, but now we will specify Spearman.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(dataset_name$variable_1, dataset_name$variable_2, method = \"spearman\") \ncor(dataset_name, method = \"spearman\") \n```\n:::\n\n\n\n## Example\n\n- Let's find the Spearman correlation of the golf data.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(golf$distance, golf$speed, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9277782\n```\n\n\n:::\n:::\n\n\n\n- Spearman's correlation is $r_{\\text{S}} = 0.93$.\n\n    - This is still a strong, positive correlation.\n\n- Compare this to Pearson's correlation, $r = 0.94$.\n\n    - In this dataset, Spearman's correlation is not vastly different.\n\n    - While we see that Spearman's correlation is smaller than Pearson's correlation, this is not always the case.\n\n## Conclusions\n\n- Correlation allows us to quantify the relationship between two variables without units.\n\n    - It does not matter if $x$ and $y$ have the same units! Correlation is unitless.\n    \n- The closer to $-1$ or $1$, the stronger the relationship.\n\n    - As correlation approaches $-1$ or $1$, $x$ is better at predicting $y$.\n    \n- For fun: play [Guess the Correlation](http://www.guessthecorrelation.com).\n\n## Introduction: Simple Linear Regression\n\n- We have discussed quantifying the relationship between two continuous variables.\n\n- Recall that the correlation describes the strength and the direction of the relationship.\n\n    - Pearson's correlation: describes the linear relationship; assumes normality of both variables.\n    \n    - Spearman's correlation: describes the monotone relationship; assumes both variables are at least ordinal.\n    \n- Further, recall that correlation is *unitless* and bounded to $[-1, 1]$.\n\n## Introduction: Simple Linear Regression\n    \n- Now we will discuss a different way of representing/quantifying the relationship.\n\n    - We will now construct a line of best fit, called the ordinary least squares (OLS) regression line. \n\n- Using simple linear regression, we will model $y$ (the outcome) as a function of $x$ (the predictor).\n\n    - This is called simple linear regression becaause there is only one predictor.\n    \n    - Next week, we will venture into multiple regression, which has mulitple predictors.\n\n## Simple Linear Regression\n\n- **Population regression line**: \n\n$$ y = \\beta_0 + \\beta_1 x + \\varepsilon $$\n\n- $\\beta_0$ is the $y$-intercept.\n\n- $\\beta_1$ is the slope describing the relationship between $x$ and $y$.\n    \n- $\\varepsilon$ (estimated by $e$) is the error term; remember, from ANOVA (ðŸ˜±): \n\n$$\\varepsilon \\overset{\\text{iid}}{\\sim} N(0, \\sigma^2)$$\n    \n## Simple Linear Regression\n\n- **Population regression line**: \n\n$$ y = \\beta_0 + \\beta_1 x + \\varepsilon $$\n    \n- **Sample regression line**: \n\n$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + e$$\n\n- $\\hat{y}$ estimates $y$.\n\n- $\\hat{\\beta}_0$ estimates $\\beta_0$.\n\n- $\\hat{\\beta}_1$ estimates $\\beta_1$.\n    \n- $e$ estimates $\\varepsilon$.\n\n## Simple Linear Regression\n\n- We first calculate the slope, $\\hat{\\beta}_1$\n\n$$ \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n x_i y_i - \\frac{\\sum_{i=1}^n x_i \\sum_{i=1}^n y_i}{n}}{\\sum_{i=1}^n x_i^2 - \\frac{\\left(\\sum_{i=1}^n x_i\\right)^2}{n}} = r \\frac{s_y}{s_x} $$\n\n- $r$ is the Pearson correlation,\n- $s_x$ is the standard deviation of $x$, and\n- $s_y$ is the standard deviation of $y$\n  \n## Simple Linear Regression\n\n- Then we can calculate the intercept, $\\hat{\\beta}_0$ \n\n$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}$$ where\n\n  - $\\bar{x}$ is the mean of $x$, and\n  - $\\bar{y}$ is the mean of $y$. \n  \n## Simple Linear Regression\n\n- We already know the syntax for modeling!\n\n    - ANOVA = regression ðŸ˜Ž\n    \n- We will use the `lm()` function to define the model and `summary()` to see the results.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(outcome_variable ~ predictor_variable, data = dataset_name)\nsummary(m)\n```\n:::\n\n\n\n## Example\n\n- Recall the golf data,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ngolf <- tibble(speed = c(100, 102, 103, 101, 105, 100, 99, 105), \n               distance = c(257, 264, 274, 266, 277, 263, 258, 275))\nhead(golf, n=3)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"speed\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"distance\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"100\",\"2\":\"257\"},{\"1\":\"102\",\"2\":\"264\"},{\"1\":\"103\",\"2\":\"274\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n\n## Example\n\n- Let's now construct the simple linear regression model.\n\n    - We want to model distance as a function of speed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(distance ~ speed, data = golf)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = distance ~ speed, data = golf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8136 -2.0195  0.3542  2.0619  3.6881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -55.7966    48.3713  -1.154  0.29257    \nspeed         3.1661     0.4747   6.670  0.00055 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.883 on 6 degrees of freedom\nMultiple R-squared:  0.8811,\tAdjusted R-squared:  0.8613 \nF-statistic: 44.48 on 1 and 6 DF,  p-value: 0.0005498\n```\n\n\n:::\n:::\n\n\n\n- This results in the model, \n\n$$\\hat{\\text{distance}} = -55.80 + 3.17 \\text{ speed}$$\n\n## Interpretations\n\n- We need to provide interpretations of \n\n- **Interpretation of the slope, $\\hat{\\beta}_1$**\n\n    - For a [$k$] [units of $x$] increase in [$x$], we expect [$y$] to [increase or decrease] by [$k \\times |\\hat{\\beta}_1|$] [units of $y$].\n\n- **Interpretation of the $y$-intercept, $\\hat{\\beta}_0$**\n\n    - When [$x$] is 0, we expect the mean of [$y$] to be [$\\hat{\\beta}_0$].\n\n    - Caveat:\n\n        - It does not always make sense for $x=0$. In this situation, we do not interpret the $y$-intercept.\n        \n        - Some applications (e.g., psychology) will center the $x$ variable around its mean. Then, when $x=0$, we are interpreting in terms of the \"average value of $x$.\"\n\n## Example\n\n- Recall the regression line from the golf pro example, $$\\hat{\\text{distance}} = -55.80 + 3.17 \\text{ speed}$$\n\n    - For a 1 mph increase in club-head speed, we expect the distance the golf ball travels to increase by 3.17 yards.\n\n    - For a 5 mph increase in club-head speed, we expect the distance the golf ball travels to increase by 15.83 yards.\n\n    - When the club-head speed is 0 mph, the average distance the golf ball travels is -55.797 yards.\n\n## Hypothesis Testing for $\\beta_1$\n\n- First, the residual: \n\n$$e_i = y_i - \\hat{y}_i$$\n\n- Then, we find the standard error of the estimate, $s_e$, \n\n$$s_e = \\sqrt{\\frac{\\sum_{i=1}^n e_i^2}{n-2}}$$\n\n- Finally, we can find the standard error of $\\hat{\\beta}_1$, $s_{\\hat{\\beta}_1}$,\n\n$$s_{\\hat{\\beta}_1} = \\frac{s_e}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}} = \\frac{s_e}{s_x \\sqrt{n-1}}$$\n\n## Hypothesis Testing for $\\beta_1$\n\n- **Hypotheses**\n\n    - $H_0: \\ \\beta_1 = 0$ \n    - $H_1: \\ \\beta_1 \\ne 0$\n\n- **Test Statistic**\n\n    - $t_0 = \\frac{\\hat{\\beta}_1}{s_{\\hat{\\beta}_1}} = \\frac{\\hat{\\beta}_1}{\\text{SE of }\\hat{\\beta}_1}$\n\n- ***p*-Value**\n\n    - $p = 2 \\times P[t_{n-2} \\ge |t_0|]$\n\n- **Rejection Region**\n\n    - Reject $H_0$ if $p<\\alpha$.\n    \n## Example\n\n- From the golf example,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = distance ~ speed, data = golf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8136 -2.0195  0.3542  2.0619  3.6881 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -55.7966    48.3713  -1.154  0.29257    \nspeed         3.1661     0.4747   6.670  0.00055 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.883 on 6 degrees of freedom\nMultiple R-squared:  0.8811,\tAdjusted R-squared:  0.8613 \nF-statistic: 44.48 on 1 and 6 DF,  p-value: 0.0005498\n```\n\n\n:::\n:::\n\n\n\n- $t_0$ for speed is 6.67\n\n    - Slope for speed is 3.1661 and the SE of the slope for speed is 0.4747\n\n    - We can see that 3.1661/0.4747 = 6.67\n\n- The corresponding $p$-value is $p = 0.00055$, which we will represent as $p < 0.001$.\n\n## Example\n\n- **Hypotheses**\n\n    - $H_0: \\ \\beta_{\\text{speed}} = 0$ \n    - $H_1: \\ \\beta_{\\text{speed}} \\ne 0$\n\n- **Test Statistic**\n\n    - $t_0 = 6.67$\n\n- ***p*-Value**\n\n    - $p < 0.001$\n\n- **Rejection Region**\n\n    - Reject $H_0$ if $p<\\alpha$; $\\alpha=0.05$\n    \n## Confidence Interval for $\\beta_1$\n\n- Recall that a point estimate by itself does not tell us how good our estimation is.\n\n- Thus, we are also interested in finding the confidence interval for $\\beta_1$.\n\n- $(1-\\alpha)100\\%$ CI for $\\beta_1$: $$ \\hat{\\beta}_1 \\pm t_{\\alpha/2,n-2} s_{\\hat{\\beta}_1}$$ where the SE of the slope, $s_{\\hat{\\beta}_1}$, is as defined earlier.\n\n- The R syntax also reuses the model results in the `confint()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m) # for 95% CI\nconfint(m, level = conf_level) # for other levels\n```\n:::\n\n\n\n## Example\n\n- Let's find the confidence intervals for the golf data,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconfint(m) # 95% CI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  2.5 %    97.5 %\n(Intercept) -174.157039 62.563818\nspeed          2.004539  4.327664\n```\n\n\n:::\n\n```{.r .cell-code}\nconfint(m, level = 0.90) # 90% CI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    5 %     95 %\n(Intercept) -149.790863 38.19764\nspeed          2.243664  4.08854\n```\n\n\n:::\n:::\n\n\n\n- The 95% CI for $\\beta_{\\text{speed}}$ is $(2.00, 4.33)$.\n- The 90% CI for $\\beta_{\\text{speed}}$ is $(2.24, 4.09)$.\n\n## Wrap Up\n\n- Today we discussed the *modeling* of continuous data.\n\n- We have discussed simple linear regression, which means we have a *single predictor* in the model.\n\n- Next week, we will extend to multiple regression (more than one predictor).\n\n- Regression is ANOVA; ANOVA is regression.\n\n    - Keep this in mind if you are asked to assess assumptions.\n\n\n\n\n\n\n\n",
    "supporting": [
      "W08-L1-simple-linear-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}